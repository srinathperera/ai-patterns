| Pattern Name | Description |
| :--- | :--- |
| **Advanced LLM Prompting** | Prompting techniques that optimize model performance, such as iterative Prompting (for MT), template-Based Prompting for controlled generation, role Prompting, prompt Optimization, and emotion prompting. |
| **Forecasting with Classical Models** | Machine learning and deep learning architectures typically trained from scratch, excluding LLM/SLM fine-tuning. Includes logistic regression, SVMs, decision trees, clustering, and foundational structures like CNNs and RNNs. |
| **Evaluating LLM Results** | Techniques for assessing performance, such as LLM-as-a-Judge (LLMEval), exemplar selection, multi-agent debate systems, adversarial evaluation, round-trip consistency filtering, and role-based evaluation. |
| **LLM-based Multimodal Generative Prompting** | Preprocessing and prompting techniques for multimodal data (image, video, audio), including segmentation, 3D prompting, and "Chain of Images" to guide generative outputs. |
| **Model Abstraction** | An abstraction layer that decouples application logic from specific LLM implementations, allowing developers to seamlessly swap models like Gemini, GPT, or Llama. |
| **Agent Architecture** | Techniques enabling multiple agents to work together or AI applications composed of multiple collaborating agents. |
| **Preprocessing Text and Numerical Data** | Data cleanup, feature extraction, or feature engineering techniques designed to improve the performance of both LLMs and classical models. |
| **Retrieval Augmented Generation (RAG)** | Enhancing RAG systems through dynamic pipelining, iterative retrieval, self-reflection, and integration of external knowledge bases like knowledge graphs. |
| **Using Tools with LLMs** | Facilitating model interaction with MCP and external software, focusing on API integration, code generation, parallel tool execution, and structured action spaces. |
| **LLM-based User Intent Extraction** | Specific techniques used to accurately identify, classify, and process user intent within a system. |
| **LLM Fine-Tuning, Training & Alignment** | Techniques for fine-tuning LLMs or SLMs, including RLHF, SFT, GRPO, DPO, and the underlying logic supporting these alignment methods. |
| **LLM-based Context Management** | Handling information over time through index hot-swapping, memory augmentation, and query complexity classification to manage long- and short-term data. |
| **Enabling Reliability, Explainability, or Robustness** | Increasing trustworthiness through confidence estimation, bias mitigation, and interpretability tools like counterfactual explanations and feature importance plots. |
| **LLM-based Planning, XoT, ReAct, or Reasoning** | Complex reasoning and task decomposition frameworks, including self-correction loops, Chain-of-Thought (CoT), Tree-of-Thought (ToT), and XoT. |
| **MLOps** | Techniques that integrates machine learning systems with DevOps principles to automate the end-to-end lifecycle of model development, deployment, and monitoring, ensuring scalable, reliable, and reproducible production environments. |