path,verified_pattern,code_summary
mcp-agent/cluster_70.py,Using Tools with LLMs,"This code implements an Augmented LLM pattern, enabling agentic systems to enhance base LLMs (OpenAI, Anthropic) with external capabilities like conversational memory and dynamic tool use. It features iterative generation for multi-turn interactions, where LLMs autonomously call tools, process results, and manage conversation history. Additionally, it supports structured output generation via JSON schemas for reliable data extraction and integrates comprehensive tracing for observability of AI interactions."
mcp-agent/cluster_70.py,Using Tools with LLMs,"This code implements a robust Remote Procedure Call (RPC) client pattern (`GenesisRPCClientTemplate`) for interacting with remote AI services, handling secure communication, retries, and timeouts. A key AI pattern demonstrated is the `RemoteGenerator`, which enables efficient, on-demand streaming and iteration over sequences of AI-generated data or results from a remote source. This infrastructure is crucial for distributed AI workloads, allowing clients to consume iterative outputs from remote models or data pipelines without loading entire datasets locally."
AIlice/cluster_24.py,Using Tools with LLMs,"This code implements a modular agentic architecture, enabling AI systems to dynamically integrate and leverage external tools for diverse tasks. It demonstrates patterns for external knowledge retrieval using vector databases (Weaviate), multimodal processing (speech-to-text, text-to-speech), and AI-driven automation through web browsing, script execution, and direct computer interaction. The system facilitates intelligent tool-use by dynamically generating prompts with tool descriptions and schemas, allowing AI agents to perform complex function calls."
AIlice/cluster_25.py,Using Tools with LLMs,"The code implements a modular tool-use pattern, where distinct classes like `ADuckDuckGo`, `AGoogle`, `AArxiv`, and `AScripter` expose specific actions and prompts for an AI agent to invoke. It incorporates a robust session management system, enabling stateful interactions and incremental consumption of large outputs through scrollable pages. Furthermore, the `AScripter` module provides an agentic execution environment for running bash and Python code, supporting dynamic code interpretation."
AIlice/cluster_30.py,Using Tools with LLMs,"This code implements a robust **tool-use pattern** for an AI agent, defining a comprehensive set of browser actions (e.g., BROWSE, SCROLL-DOWN, EXECUTE-JS) with explicit prompts and parameter definitions via the `ModuleInfo` method and `functions` dictionary. It further employs a **context-aware tool selection pattern**, dynamically instantiating specialized browser types (web, PDF, text, file) based on the input, ensuring the AI interacts appropriately with diverse content within a strictly defined, headless environment."
AIlice/cluster_33.py,Using Tools with LLMs,"This code defines an `AProcessor` class, representing an AI agent within a multi-agent system, capable of dynamically invoking tools and managing sub-agents. It employs a sophisticated prompt engineering pattern, building structured prompts for an integrated LLM based on conversational context and registered actions. The system further supports dynamic extensibility by loading external modules and new agent types, enhancing its capabilities and enabling persistent state management for agents and their interactions."
AIlice/cluster_35.py,Using Tools with LLMs,"This code implements a **tool/function calling pattern** through an `AInterpreter` that dynamically registers regex patterns and corresponding actions to parse and execute domain-specific commands and object creations from text. It also features **multimodal input processing** within `AConversations`, extracting code snippets and handling various media types or variable references embedded in messages. A shared environment (`env`) facilitates **contextual state management**, allowing the AI to store, retrieve, and manipulate variables and objects throughout a conversation."
AIlice/cluster_4.py,Using Tools with LLMs,"This code implements a multi-agent AI system, where specialized agents like `APromptResearcher` and `APromptCoderProxy` are equipped with distinct function-calling capabilities. A core pattern involves dynamic prompt engineering, integrating contextual information and retrieval-augmented generation (RAG) from stored data. This enables agents to delegate tasks, execute code (Bash/Python), and interact with external tools for adaptive problem-solving."
AIlice/cluster_7.py,Using Tools with LLMs,"The code implements an agentic AI pattern, orchestrating an `AProcessor` within a `TaskSession` that manages its lifecycle and state transitions via a state machine. This processor integrates a pool of Large Language Models and leverages a modular architecture by registering various external services (e.g., search, coding, speech) as tools. It further employs a `APromptsManager` to handle specialized prompts, enabling sophisticated prompt engineering for diverse AI tasks."
AgentBench/cluster_10.py,Using Tools with LLMs,"This code defines an `HTTPAgent` that implements a pattern for interacting with external AI models, utilizing a configurable `prompter` to manage and format conversational history for API requests. It incorporates specific AI-centric robustness patterns, including retries for API calls and a heuristic `check_context_limit` function to detect and handle large language model context window overflow errors. This approach demonstrates an agent-based strategy for integrating and managing interactions with AI services, addressing their unique input and error characteristics."
AgentBench/cluster_11.py,Using Tools with LLMs,"This code implements a robust configuration pattern for AI agent-task assignments, allowing flexible input for specifying which agents perform which tasks. It enforces strict consistency by validating that all assigned agents and tasks are properly defined and have associated concurrency settings. Furthermore, it incorporates a resource optimization pattern by automatically pruning unused agent and task definitions from the configuration, ensuring only relevant components are processed."
AgentBench/cluster_20.py,Using Tools with LLMs,"This code implements an interactive AI agent pattern, where an agent engages with an `ALFWorld` environment through a tool-use mechanism. The agent receives observations and available actions, then generates tool calls to perform actions, with the system providing task instructions via prompt engineering and evaluating performance using a reward mechanism. This setup facilitates goal-oriented AI tasks within a simulated environment, handling iterative planning and action execution."
AgentBench/cluster_7.py,Using Tools with LLMs,"This code implements a semantic parsing pattern, translating Lisp-like symbolic expressions into SPARQL queries for knowledge graphs. It specifically handles complex AI query patterns such as superlatives (ARGMAX/ARGMIN), temporal constraints (TC), and aggregation (COUNT). The system also incorporates variable unification to resolve co-references within the generated query, enabling robust knowledge base question answering."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_5.py,Using Tools with LLMs,"This code defines a Reinforcement Learning environment within the CARLA simulator, implementing standard `reset()` and `step()` methods for agent interaction. It constructs a multi-modal state representation from visual and numerical navigation data, and employs reward shaping to guide an autonomous vehicle agent towards desired driving behaviors. The environment further integrates AI-controlled non-player characters (pedestrians and vehicles) to create dynamic and realistic simulation scenarios."
Copilot-Agent-365/cluster_2.py,Using Tools with LLMs,"This code implements a rule-based conversational AI pattern, providing canned responses by matching user input against pre-defined conversation flows stored in JSON files. It further demonstrates an agent orchestration pattern, dynamically loading and executing specialized sub-agents from a GitHub repository and resolving their parameters from user input or context. The system also incorporates a pattern for rich, structured output generation, formatting pre-rendered or agent-executed data into various display types like dashboards and email drafts."
EvoAgentX/cluster_16.py,Using Tools with LLMs,"This code implements a client for interacting with multiple ""MCP servers,"" embodying a **tool-use pattern** for AI agents. It discovers and aggregates capabilities as structured ""tools,"" each defined with a name, description, and input schema, enabling an orchestrating AI agent to dynamically select and invoke them. The client further facilitates multi-agent interaction by integrating asynchronous tool calls into a synchronous execution context."
EvoAgentX/cluster_72.py,Using Tools with LLMs,"This code exemplifies an **AI tool-use pattern**, where an `StorageToolkit` provides an agent with modular capabilities to interact with an external file system. It demonstrates the dynamic retrieval and invocation of specific tools, such as 'save', 'read', and 'delete', to perform operations on diverse data formats including CSV, YAML, and PDF. This pattern enables AI systems to extend their functionalities by abstracting complex external interactions into manageable, callable tools."
EvoAgentX/cluster_75.py,Using Tools with LLMs,"The code demonstrates a **tool-use pattern** for browser automation, programmatically orchestrating specialized tools like navigation, input, click, and snapshot to interact with web elements identified by their semantic descriptions. It further showcases an **agentic browser automation pattern** where a large language model interprets natural language instructions to autonomously perform complex web tasks, abstracting the explicit sequencing of individual browser actions."
EvoAgentX/cluster_76.py,Using Tools with LLMs,"The code demonstrates AI patterns for dynamic code generation and execution, enabling an agent to construct and run Python code snippets or scripts. It establishes a controlled execution environment through `allowed_imports` and isolated interpreters (Python, Docker), crucial for sandboxing AI-generated instructions. This infrastructure supports AI agents in defining functions on the fly and leveraging external tools via programmatic execution."
FinceptTerminal/cluster_34.py,Using Tools with LLMs,"This code demonstrates an **Adaptive Content Generation** pattern, dynamically tailoring the user interface and available features based on the user's authentication status (guest, registered, or unknown). It employs **Intelligent Data Refresh and Caching** mechanisms to efficiently manage API interactions and optimize data display performance. Robust **API Integration and State Management** ensure reliable fetching of user profiles and usage statistics, adapting the application's behavior to real-time data."
GPT_Vuln-analyzer/cluster_6.py,Using Tools with LLMs,"The code demonstrates a pattern of **dynamic AI model integration and selection**, allowing the `JWTAnalyzer` to dispatch analysis tasks to various AI services like OpenAI, Bard, and Llama based on runtime configuration. It employs a **strategy for abstracting and managing multiple AI backends** through a `model_map` and a dedicated `AI_models` object. This enables the use of AI for advanced interpretation of decoded JWT data, with specific handling for different AI endpoints and API tokens."
GPT_Vuln-analyzer/cluster_9.py,Using Tools with LLMs,"This code demonstrates a pattern of leveraging a large language model (LLM) as an analytical engine for specialized tasks. It employs detailed prompt engineering within the `DnsAI` function to instruct the LLM to perform DNS analysis from a pentester's viewpoint, explicitly dictating a structured JSON output format. A subsequent pattern involves robust post-processing of the LLM's response using regular expressions in `dns_extract_data` to reliably parse and extract specific DNS record types, ensuring data integrity."
MassGen/cluster_4.py,Using Tools with LLMs,"This code implements robust **Tool Integration** patterns, converting external Model Context Protocol (MCP) tools into callable `Function` objects for AI agents to utilize. It heavily leverages **Resilience Patterns** such as **Circuit Breakers** and **Retry with Exponential Backoff** to manage interactions with potentially unreliable MCP backends. This ensures fault tolerance and graceful recovery from connection, timeout, and server errors during AI service orchestration."
Stride-AI-Agents/cluster_3.py,Using Tools with LLMs,"This code outlines a multi-agent AI system, orchestrating specialized `Assistant` agents capable of dynamic tool use and LLM-based planning. It employs an LLM-driven request triage pattern to intelligently route user queries to the most appropriate agent for execution. The architecture further supports conversational context management and includes an evaluation framework for assessing agent performance and planning accuracy."
Stride-AI-Agents/cluster_7.py,Using Tools with LLMs,"This code implements a multi-agent system, orchestrating a ""swarm"" of specialized assistants, each equipped with dynamically loaded tools for function calling. It employs an AI-driven agent routing pattern to triage user requests and delegate tasks to the most appropriate assistant or sub-assistant. The system further integrates AI planning, enabling assistants to generate and execute multi-step plans, including human-validated tool invocations, and features comprehensive evaluation patterns for agent performance and task delegation."
factorio-learning-environment/cluster_33.py,Using Tools with LLMs,"This code implements an **environment rendering** pattern, generating visual observations (RenderedImage) from structured data like blueprints or real-time map entities, water tiles, and resources. It incorporates **state representation** by decoding various optimized and compressed data formats (e.g., `v2-binary`, `v2`) into a unified structure for rendering. The `Render` class, designed as a `Tool`, further suggests an **agentic pattern** for AI to programmatically obtain visual information from its environment."
factorio-learning-environment/cluster_40.py,Using Tools with LLMs,"This code implements a dynamic tool-use pattern, where Python controllers are loaded and paired with Lua scripts to enable agents to interact with an external game environment via RCON. It further incorporates a robust hook-based extensibility mechanism, allowing for the registration and execution of `pre-tool` and `post-tool` callbacks around agent actions. This enables advanced AI patterns such as monitoring agent tool usage, injecting meta-reasoning, or enforcing constraints on agent behavior."
mcp-agent/cluster_61.py,Using Tools with LLMs,"This code demonstrates an augmented LLM pattern that orchestrates multi-turn interactions, integrating external tool use for dynamic information retrieval and action execution, alongside robust error handling. It incorporates conversational memory management, dynamic model selection, and structured output generation, often facilitated by tool-calling. Furthermore, the system employs an abstraction layer for type conversion, standardizing interactions with the Anthropic API and enabling flexible system prompt management."
mcp-agent/cluster_68.py,Using Tools with LLMs,"This code implements patterns for **LLM Tooling and Function Calling**, specifically transforming JSON schemas into a Google Gemini-compatible format and orchestrating both sequential and parallel tool executions. It further demonstrates **Structured Output generation** by enforcing Pydantic model adherence and manages **conversational context** through message history for augmented LLM interactions."
mcp-agent/cluster_9.py,Using Tools with LLMs,"This code implements an **agentic tool-use pattern** by defining and registering callable ""tools"" (`@mcp.tool`) with schema-validated and context-filtered parameters, enabling AI agents to discover and invoke specific functionalities or orchestrate complex AI workflows. It also manages **contextual AI interactions** by resolving user and execution-specific identities and contexts, facilitating **asynchronous AI service integration** via internal API routes for notifications, requests, and human prompts within a distributed AI system."
semantic-router/cluster_7.py,Using Tools with LLMs,"This code demonstrates robust **LLM function calling and tool use** by dynamically generating structured schemas from Python callables and Pydantic models, enabling LLMs to understand and invoke available tools. It further implements **LLM-driven argument extraction and validation**, where LLMs are prompted to extract function parameters from user queries, followed by rigorous validation against the generated schemas. Additionally, a **semantic routing** pattern is employed to direct queries to specific functions or augment prompts with context, facilitating a hybrid execution model."
FinceptTerminal/cluster_62.py,Retrieval Augmented Generation(RAG),"This code primarily demonstrates patterns for robust **data ingestion and preparation**, foundational stages in any AI pipeline. It includes utilities for **reliable external data acquisition** from web sources via API calls and HTML parsing (`make_request`, `get_schema_filelist`). Furthermore, it incorporates **data sanitization and formatting** (`safe_format`) to ensure data quality before it can be utilized by AI models."
HuixiangDou/cluster_4.py,Retrieval Augmented Generation(RAG),"This code implements a multi-stage AI pipeline, beginning with an LLM-based question classification system that leverages prompt engineering to score and filter conversational turns. Subsequently, it employs another LLM for coreference resolution within chat contexts, utilizing multi-turn prompting to determine the necessity of resolution and perform text transformation. A human-in-the-loop annotation process is integrated to generate ground truth and evaluate the performance of these LLM-driven tasks using standard classification metrics."
Stride-AI-Agents/cluster_0.py,Retrieval Augmented Generation(RAG),"This code implements a **semantic search** pattern by transforming natural language queries into **vector embeddings** using an external embedding model. These embeddings are then utilized to perform similarity searches against a **vector database** (Qdrant), enabling the retrieval of contextually relevant documents. This forms a crucial retrieval component often found in **Retrieval Augmented Generation (RAG)** architectures."
factorio-learning-environment/cluster_2.py,Retrieval Augmented Generation(RAG),"This code generates training data following a sequence-to-sequence pattern, designed to teach an AI model hierarchical navigation and completion. It constructs input prompts representing a current path within a tree structure and corresponding outputs predicting direct children, the next sibling, or an end-of-branch token. These structured examples are then formatted for fine-tuning large language models, enabling them to understand and generate hierarchical information in a conversational context."
AIlice/cluster_5.py,Retrieval Augmented Generation(RAG),"This code implements an AI agent utilizing a robust function-calling pattern, enabling it to interact with a browser for actions like reading, scrolling, and executing JavaScript. It employs a Retrieval Augmented Generation (RAG) pattern by storing browsed document content and semantically recalling relevant information to augment the LLM's context. Furthermore, the system dynamically constructs prompts, integrating conversation history and retrieved data while managing context window limits for efficient information processing."
ArxivDigest/cluster_0.py,Retrieval Augmented Generation(RAG),"This code demonstrates a pattern of **Large Language Model (LLM) integration** for document relevance scoring, specifically using prompt engineering to construct detailed inputs for a GPT model. It iteratively processes papers in batches, dynamically encoding multiple abstracts and a query into a single prompt to manage context windows. The system then performs **structured output parsing** on the LLM's response, extracting relevancy scores, and includes logic for **hallucination detection** and score-based filtering of results."
Copilot-Agent-365/cluster_1.py,Retrieval Augmented Generation(RAG),"This code implements an agentic AI system where an `Assistant` orchestrates various `known_agents` using OpenAI's function calling for dynamic tool use. It features a `GitHubAgentLibraryManager` for discovering, installing, and grouping agents from an external repository, enabling dynamic capability expansion. Furthermore, the system incorporates robust contextual memory management, distinguishing between shared and user-specific memories to maintain personalized and persistent conversation context."
EvoAgentX/cluster_1.py,Retrieval Augmented Generation(RAG),"The code implements comprehensive Retrieval-Augmented Generation (RAG) patterns, featuring multimodal data ingestion, diverse chunking strategies (semantic, hierarchical, simple), and various embedding models (OpenAI, Azure, HuggingFace, Voyage, Ollama) for vector and graph indexing and retrieval. It also includes an agentic web interaction framework, enabling AI to programmatically navigate and interact with web pages. Furthermore, the system incorporates an evolutionary prompt optimization pattern for automatically improving AI agent instructions through iterative generation and evaluation."
HuixiangDou/cluster_34.py,Retrieval Augmented Generation(RAG),"The code implements a conversational AI agent pattern, utilizing `LarkAgent` and `WechatAgent` to process multimodal user inputs (text and images) from different platforms. It integrates with a knowledge retrieval system, fetching contextual information from a `QaLibCache` based on `feature_store_id` for AI processing. The architecture supports asynchronous AI inference, with agents managing query lifecycles and delivering responses back to users."
HuixiangDou/cluster_60.py,Retrieval Augmented Generation(RAG),"The `FeatureStore` class orchestrates a hybrid retrieval-augmented generation (RAG) system, performing multi-modal feature extraction. It generates dense vector embeddings for documents using an `embedder` and FAISS for efficient similarity search, alongside sparse features for code via BM25Okapi. Additionally, the system incorporates advanced text chunking, named entity indexing, and QA pair processing to prepare diverse data for AI pipelines."
HuixiangDou/cluster_65.py,Retrieval Augmented Generation(RAG),"This code implements a Retrieval Augmented Generation (RAG) pattern, initializing a `FeatureStore` with an `embedder` to manage features derived from scanned files. It then leverages a `CacheRetriever` and `compression_retriever` to efficiently fetch relevant documents for incoming queries, structuring these retrieved documents as candidates with relevance scores for potential downstream AI tasks."
HuixiangDou/cluster_69.py,Retrieval Augmented Generation(RAG),"This code implements a Large Language Model (LLM) reranking pattern, utilizing a pre-trained `bge-reranker-v2-minicpm-layerwise` model to score and reorder candidate passages based on their relevance to a given query. It employs instruction-tuned LLM inference by crafting a specific prompt for each query-passage pair and extracts relevance scores from a designated intermediate layer of the model for ranking."
Stride-AI-Agents/cluster_5.py,Retrieval Augmented Generation(RAG),"This code demonstrates AI patterns for intelligent web content processing, primarily focusing on text segmentation and semantic information extraction. It employs NLP-based sentence chunking for refined text partitioning and leverages cosine similarity via `CosineStrategy` to identify and extract semantically similar text blocks, optionally guided by a semantic filter. A rule-based `RegexChunking` strategy is also presented for flexible text segmentation."
chipper/cluster_11.py,Retrieval Augmented Generation(RAG),"The code implements a Retrieval-Augmented Generation (RAG) pattern, where an `RAGQueryPipeline` integrates external knowledge retrieval with LLM generation, delivering responses via a streaming callback mechanism. It also features an `OllamaProxy` that acts as a unified gateway for streaming and non-streaming interactions with an LLM service, encompassing core generation, chat, embedding, and comprehensive model lifecycle management operations. This design emphasizes asynchronous processing and robust error handling for continuous streaming of AI model outputs."
chipper/cluster_8.py,Retrieval Augmented Generation(RAG),"This code implements a Retrieval Augmented Generation (RAG) pipeline, primarily focusing on the ingestion and embedding of documents. It employs a `DocumentProcessor` for pre-processing, including chunking strategies (`split_by`, `split_length`) and blocklist filtering, before using an `RAGEmbedder` to generate vector embeddings via an external model (e.g., Ollama). These embeddings are then indexed into a vector store like Elasticsearch, with integrated metrics tracking for pipeline observability."
chipper/cluster_9.py,Retrieval Augmented Generation(RAG),"This code implements AI patterns for document processing, specifically focusing on text embedding generation and management. It supports configurable embedding providers (Ollama, Hugging Face) and models, alongside detailed parameters for text chunking crucial for preparing data for vectorization. The integration with Elasticsearch for storage and retrieval of these embeddings indicates a design for semantic search or retrieval-augmented generation systems."
evadb/cluster_39.py,Retrieval Augmented Generation(RAG),"The code demonstrates patterns for integrating and managing AI models as User-Defined Functions (UDFs) within a data processing framework, enabling tasks like text filtering, image classification, and saliency detection directly on multimedia. It implements vector indexing (FAISS, QDRANT) with feature extractors for efficient similarity search on unstructured data, complemented by query optimization rules tailored for AI-driven workloads. The system also supports dynamic function creation from various AI frameworks (HuggingFace, Ludwig, Sklearn) and incorporates document chunking for NLP/LLM data preparation."
fast-graphrag/cluster_13.py,Retrieval Augmented Generation(RAG),"This code defines core AI patterns for knowledge graph management, including structured representation of entities and relationships, along with mechanisms for querying and editing them. It implements a Retrieval Augmented Generation (RAG) pattern by assembling a rich context from documents, chunks, entities, and relations, complete with relevance scoring and context window truncation. This comprehensive approach facilitates structured query responses by integrating retrieved information with generated content."
fast-graphrag/cluster_17.py,Retrieval Augmented Generation(RAG),"This code implements a Retrieval-Augmented Generation (RAG) pattern, leveraging `OpenAIEmbeddingService` to convert text into embeddings for storage in an `HNSWVectorStorage`. It performs semantic search to retrieve relevant context, which is then dynamically injected into a structured prompt for an `OpenAILLMService`. This architecture enables the LLM to generate grounded responses by augmenting its knowledge with specific, retrieved information."
optillm/cluster_23.py,Retrieval Augmented Generation(RAG),"This code implements an AI agent tool for web search, embodying a Retrieval Augmented Generation (RAG) pattern. It leverages Natural Language Understanding (NLU) through pattern matching to extract explicit and implicit search queries from an initial prompt. The retrieved web search results are then formatted and used to augment the original query, providing external context for subsequent AI processing."
scattertext/cluster_115.py,Retrieval Augmented Generation(RAG),"This code implements a pattern for generating contextualized embeddings from a corpus using pre-trained transformer models and tokenizers. It extracts span-based embeddings for specific terms and text segments by aligning sub-token offsets with original text, subsequently aggregating these into running statistics to build aggregate semantic representations for keywords and categories across a document collection."
semantic-router/cluster_12.py,Retrieval Augmented Generation(RAG),"This code implements a pluggable vector search and storage system, offering integrations with Pinecone, Qdrant, PostgreSQL, and a local in-memory index. It facilitates semantic routing by storing vector embeddings alongside associated routes, utterances, and metadata, enabling efficient retrieval and filtering for AI applications. The system also incorporates asynchronous operations for scalable performance and leverages vector databases for dynamic configuration management of AI components."
semantic-router/cluster_2.py,Retrieval Augmented Generation(RAG),"The codebase implements a **vector search** framework, abstracting various vector database backends (`PineconeIndex`, `QdrantIndex`, `PostgresIndex`, `LocalIndex`, `HybridLocalIndex`) for storing and querying embeddings. It integrates **embedding generation** through diverse encoders (`BedrockEncoder`, `OpenAIEncoder`) and applies these to **semantic routing** (`BaseRouter`, `SemanticRouter`) for input classification, notably featuring a **hybrid retrieval** pattern in `HybridRouter` that combines dense and sparse vectors. The architecture also supports **asynchronous operations** and robust **configuration management** for dynamic routing logic."
semantic-router/cluster_20.py,Retrieval Augmented Generation(RAG),"This code implements an AI pattern for **sparse embedding generation** by integrating with the Aurelio Platform's API. It utilizes an **asymmetric encoding strategy** for queries and documents, a common pattern in information retrieval, and provides both **synchronous and asynchronous inference** capabilities for efficient AI model interaction."
semantic-router/cluster_21.py,Retrieval Augmented Generation(RAG),"This code implements a **Semantic Router** pattern, utilizing **dense vector embeddings** (e.g., OpenAIEncoder) and a configurable **vector index** (e.g., Pinecone, Qdrant) to semantically classify and route input queries. It supports **LLM orchestration** for dynamic route execution via function schemas and includes robust **data synchronization** capabilities to manage consistency between local route definitions and remote vector stores."
vigil-llm/cluster_2.py,Retrieval Augmented Generation(RAG),"This code establishes a modular AI pipeline, integrating a `VectorDB` and an `embedder` for semantic processing and retrieval-augmented capabilities. It employs a pluggable ""scanner"" architecture, where various AI-powered modules can be dynamically configured and instantiated, optionally leveraging the vector database and embedder for tasks like input/output analysis or moderation. This design facilitates a flexible and extensible framework for managing AI system interactions."
vigil-llm/cluster_7.py,Retrieval Augmented Generation(RAG),"This code implements a vector database pattern, leveraging ChromaDB for persistent storage and retrieval of high-dimensional embeddings. It features a pluggable embedding strategy, allowing selection between OpenAI's API and local Sentence Transformer models to generate these vectors. This architecture facilitates semantic search and retrieval, enabling queries to find semantically similar documents based on vector similarity."
vigil-llm/cluster_10.py,Retrieval Augmented Generation(RAG),"This code implements a modular AI processing system that integrates vector databases and embedders for semantic operations, likely supporting retrieval-augmented generation or similarity search. It features an extensible pipeline of input and output scanners, dynamically injecting AI-specific dependencies like vector databases and embedders based on scanner requirements. The architecture, managed by a `ScannerRegistry`, also incorporates `CanaryTokens` for security monitoring within the AI workflow."
scattertext/cluster_11.py,Preprocessing Text and Numerical Data,"This code implements a Natural Language Processing (NLP) pattern for feature selection within a Term-Document Matrix. It employs statistical NLP techniques by calculating unigram and bigram frequencies and probabilities. A core AI pattern is the application of Pointwise Mutual Information (PMI) to filter out statistically insignificant bigrams, combined with frequency-based term filtering, to refine the feature set for subsequent AI models."
nuplan-devkit/cluster_176.py,Preprocessing Text and Numerical Data,"The code implements several data curation and filtering patterns crucial for autonomous driving AI datasets. It includes filtering scenarios based on the fractional presence of specific LiDAR point cloud tokens, selecting scenarios exhibiting particular ego vehicle dynamics (e.g., non-stationary, starting, stopping), and ensuring contextual relevance by checking for route-intersecting roadblocks. These patterns are essential for preparing targeted and high-quality datasets for training and evaluating perception, prediction, and planning models."
Automatic_Speech_Recognition/cluster_0.py,Preprocessing Text and Numerical Data,"This code implements a rule-based natural language processing (NLP) pattern for numeral conversion, specifically transforming Chinese numeral strings into Arabic numerals. It employs linguistic parsing by segmenting the input based on explicit Chinese character markers for place values (e.g., '十', '百', '千', '万', '亿') and decimal points. This system leverages a predefined set of character-based rules to systematically extract and reconstruct numerical values, representing a form of symbolic AI for text normalization."
Automatic_Speech_Recognition/cluster_18.py,Preprocessing Text and Numerical Data,"The code implements a rule-based natural language generation (NLG) pattern, specifically for converting numerical values into their Chinese linguistic expressions. It employs a hierarchical decomposition strategy, processing numbers by sections and individual digits, and utilizes explicit character and unit lookup tables to apply complex linguistic rules for Chinese number representation. This deterministic approach reflects a symbolic AI pattern, where linguistic transformations are achieved through predefined algorithms rather than learned models."
CAAFE/cluster_2.py,Preprocessing Text and Numerical Data,"This code implements an iterative AI pattern for automated feature engineering, where a Large Language Model (LLM) generates Python code snippets. These generated code blocks are executed and evaluated against performance metrics in a cross-validation setup. A feedback loop is established, informing the LLM's subsequent prompts based on execution results and performance improvements, enabling iterative refinement of generated features."
evadb/cluster_72.py,Preprocessing Text and Numerical Data,"This code implements a data processing pipeline designed for AI workloads, leveraging a `Batch` data structure for efficient handling of tabular or frame-based data. A core AI pattern is the `FunctionExpression`, which integrates and executes AI models or custom functions, supporting GPU acceleration and output projection. This pattern also incorporates a caching mechanism to optimize repeated inferences and includes cost instrumentation for performance monitoring of AI functions within the execution engine."
3DOD_thesis/cluster_3.py,Preprocessing Text and Numerical Data,"This code implements an **angular discretization** pattern, specifically **binning**, by mapping a continuous angle into one of four discrete categories. This serves as a **feature engineering** step, transforming raw angular data into a quantized representation suitable for histogram-based features or models requiring discrete inputs."
AIlice/cluster_36.py,Preprocessing Text and Numerical Data,"The code implements a pattern-matching interpreter, a fundamental AI pattern for processing structured input. It dynamically generates regular expression-based parsing rules for various constructs, including data types, variable operations, function calls, and object instantiations. This forms a rule-based system where recognized patterns are mapped to specific actions for evaluation and execution."
AgentBench/cluster_9.py,Preprocessing Text and Numerical Data,"This code implements a semantic parsing pattern, translating a graph-based query representation into a LISP-like formal query language. It employs recursive graph traversal and manipulation to construct compositional queries, handling relations, node functions (e.g., comparisons), and aggregations like COUNT for knowledge graph interaction. This system effectively converts structured knowledge graph queries into executable logical forms."
ArxivDigest/cluster_2.py,Preprocessing Text and Numerical Data,"This code implements an AI pattern for **information retrieval and content filtering**, specifically by selecting papers relevant to ""Computation and Language"" and ""Artificial Intelligence"" subjects. It then leverages a **Large Language Model (LLM)**, identified as 'gpt-3.5-turbo-16k', to **generate relevance scores** for these filtered papers, employing **prompt engineering techniques** via parameters like `num_paper_in_prompt`, `temperature`, and `top_p` to guide the scoring process."
Automatic_Speech_Recognition/cluster_4.py,Preprocessing Text and Numerical Data,"This code implements a deep learning model based on multi-layer bidirectional recurrent neural networks (BRNNs), supporting various cell types (RNN, GRU, LSTM) with optional Layer Normalization. It leverages Connectionist Temporal Classification (CTC) loss for sequence-to-sequence alignment and prediction, incorporating dropout for regularization and Adam optimization with gradient clipping. The architecture processes sequential input, concatenates forward and backward hidden states, and applies a final fully connected layer for classification."
Automatic_Speech_Recognition/cluster_6.py,Preprocessing Text and Numerical Data,"This code primarily showcases AI patterns for **data preprocessing and feature engineering** across diverse Chinese language datasets. It systematically transforms raw text, such as poetry and QA, into simplified character-Pinyin pairs, a common phonetic representation crucial for tasks like speech recognition or pronunciation modeling. Additionally, it defines output class counts for various AI tasks (e.g., phoneme, character, sequence-to-sequence) and manages dataset splits, reflecting comprehensive data preparation for NLP and ASR models."
Automatic_Speech_Recognition/cluster_7.py,Preprocessing Text and Numerical Data,"This code implements an audio feature extraction pipeline, converting `.WAV` files into scaled delta-delta features using `calcfeat_delta_delta`. It generates corresponding numerical labels by mapping phonemes or characters from annotation files, incorporating specific start/end tokens for sequence-to-sequence model preparation. The processed features and labels are then persisted as a dataset, ready for supervised AI model training."
CAAFE/cluster_8.py,Preprocessing Text and Numerical Data,"This code implements a pattern for automated dataset curation and preprocessing, systematically loading and filtering classification datasets from OpenML and Kaggle based on criteria like missing values, feature count, sample size, and number of classes. It prepares this data for AI model consumption by applying capping mechanisms, handling multiclass/binary scenarios, and transforming it into PyTorch tensors with controlled shuffling or sorting, indicating its use for deep learning benchmarks or training."
ChatSim/cluster_107.py,Preprocessing Text and Numerical Data,"The code defines a recursive `as_variable` utility that converts various Python objects (scalars, sequences, mappings) into a specialized `Variable` type. This pattern is fundamental in deep learning frameworks for ""tensorization"" or ""variable wrapping,"" ensuring all data is represented in a differentiable format. It facilitates the seamless integration of raw data into computation graphs, enabling automatic differentiation."
DLTA-AI/cluster_112.py,Preprocessing Text and Numerical Data,"This code implements a nearest neighbor distance pattern by calculating the minimum Euclidean distance from query points to a set of sample points. It leverages an optimized pairwise squared Euclidean distance computation, a fundamental metric widely used in machine learning for tasks like clustering, classification, and density estimation, leveraging vectorized operations for efficiency."
DLTA-AI/cluster_113.py,Preprocessing Text and Numerical Data,"This code implements a **nearest neighbor search pattern** using **cosine distance** as the similarity metric, a common approach for comparing high-dimensional vector representations. It incorporates **data normalization** as a preprocessing step, ensuring robust distance calculations crucial for tasks like clustering or retrieval in AI systems."
DLTA-AI/cluster_36.py,Preprocessing Text and Numerical Data,"This code implements a **spatial-aware anchor assignment pattern** for object detection, which categorizes proposals into positive, negative, or ignored based on their overlap with scaled ground truth regions. It defines a ""core"" positive region and a ""shadow"" region around each ground truth, resolving multi-match conflicts by prioritizing smaller ground truths. This pattern ensures precise sample labeling by considering both spatial proximity and hierarchical importance of ground truth objects."
DLTA-AI/cluster_40.py,Preprocessing Text and Numerical Data,"This code implements a bucket-based bounding box localization pattern, discretizing bounding box edges into a fixed number of buckets. It generates classification targets for the closest bucket and fine regression targets for offsets within selected buckets, along with corresponding weights. During inference, the system reconstructs bounding boxes by combining the highest confidence bucket prediction with its associated fine offset."
DLTA-AI/cluster_65.py,Preprocessing Text and Numerical Data,"This code implements core AI patterns for semantic segmentation, defining a `BaseSemanticHead` that manages `num_classes` and integrates a configurable `CrossEntropyLoss`. A prominent pattern is the explicit spatial resampling of feature maps and predictions via `interpolate_as`, crucial for aligning dimensions during loss computation and for generating final, correctly-sized segmentation outputs at inference."
EvoAgentX/cluster_58.py,Preprocessing Text and Numerical Data,"The code primarily implements a **Feature Engineering** pattern by calculating various technical indicators from raw stock data, which are essential inputs for financial AI models. It then utilizes these processed features and raw data to generate comprehensive technical and candlestick charts, serving as a **Data Visualization** component for analysis and potential model interpretation within an AI workflow. This structured approach facilitates the preparation and visual exploration of financial data for subsequent AI-driven insights."
FinceptTerminal/cluster_10.py,Preprocessing Text and Numerical Data,"This code implements AI patterns for geospatial data processing, specifically intelligent data filtering to select satellite imagery based on spatial, temporal, and environmental criteria like cloud cover. It then applies specialized image analysis algorithms (""evalscripts"" such as NDVI, NDWI, and Urban Index) to extract meaningful features like vegetation health and urban areas from the raw imagery. Additionally, it incorporates patterns for ingesting and retrieving real-time environmental sensor data, structured as time-series readings with geographical metadata."
FinceptTerminal/cluster_16.py,Preprocessing Text and Numerical Data,"This code exemplifies foundational AI data preparation patterns, specifically **feature extraction** and **data profiling**. It utilizes regular expressions to parse raw Databento data, extracting key features such as message types, symbols, actions, prices, and timestamps from unstructured text. The extracted information is then statistically aggregated to generate counts, ranges, and averages, providing a comprehensive overview of the dataset's characteristics crucial for subsequent AI model development."
FinceptTerminal/cluster_2.py,Preprocessing Text and Numerical Data,"This code represents an **automated data acquisition pattern** crucial for AI applications, specifically extracting structured financial event data from web sources. It functions as a **feature engineering component**, transforming raw web content into discrete, usable features like 'impact', 'actual', and 'forecast' that are essential inputs for training predictive models or other analytical AI systems."
FinceptTerminal/cluster_25.py,Preprocessing Text and Numerical Data,"This code primarily exhibits patterns of quantitative financial analysis and statistical inference. It calculates various performance metrics like information ratio and tracking error, and applies a t-test with predefined critical values to determine the statistical significance of active returns. Furthermore, it employs a rule-based heuristic for decomposing value added into allocation and selection effects, rather than a data-driven model."
FuxiCTR/cluster_5.py,Preprocessing Text and Numerical Data,"This code implements a comprehensive **feature engineering pipeline** for machine learning, systematically transforming raw data into model-ready inputs. It incorporates specific **AI patterns** such as **vocabulary-based encoding** for categorical and sequence features (managing OOV tokens, padding, and shared embeddings), **numerical normalization**, and the integration of **pretrained embeddings**. Additionally, it supports advanced categorical processing via **quantile and hash bucketing**."
HuixiangDou/cluster_1.py,Preprocessing Text and Numerical Data,"The code demonstrates patterns for integrating Large Language Models (LLMs) to process chat data. It utilizes LLM-based text classification to identify questions and performs coreference resolution by leveraging conversational history, including an LLM-driven decision to determine if resolution is needed."
HuixiangDou/cluster_53.py,Preprocessing Text and Numerical Data,"This code implements text chunking patterns fundamental for AI applications, particularly large language models and retrieval systems. It includes a `CharacterTextSplitter` for direct separation and a `RecursiveCharacterTextSplitter` that employs a recursive strategy with multiple separators to robustly segment text. This approach ensures text is broken into manageable units, optimizing for AI model context window limits and efficient information retrieval."
HuixiangDou/cluster_58.py,Preprocessing Text and Numerical Data,"This code implements a robust feature store for AI pipelines, leveraging diverse text splitting strategies (e.g., recursive, markdown-header, code-specific) and an `Embedder` for dense vector representation. It supports both dense retrieval (Faiss with embeddings) and sparse retrieval (BM25Okapi) for different document types, further enhancing information retrieval with named entity-based inverted indexing. This comprehensive approach prepares varied content like markdown, code, and QA pairs for advanced AI applications such as Retrieval Augmented Generation."
clai/cluster_27.py,Preprocessing Text and Numerical Data,"This code implements a data pre-processing pipeline that transforms raw command-line arguments into structured `FileChange` objects. It employs categorical data encoding by mapping string-based statuses and types to enumerated values. The processed and encoded state is then sent via a `ClaiClient`, representing an integration pattern for feeding structured observations to an AI agent or service."
deep_recommenders/cluster_3.py,Preprocessing Text and Numerical Data,"This code implements a critical AI pattern of data preprocessing and serialization for machine learning input pipelines. It transforms raw MovieLens dataset components into a unified structure, then serializes this data into TensorFlow's optimized `TFRecord` format. This involves defining structured features using `tf.train.Example` and `tf.train.Feature` with appropriate data types like `Int64List` and `BytesList`, essential for efficient data feeding to TensorFlow models."
factorio-learning-environment/cluster_49.py,Preprocessing Text and Numerical Data,"This code implements **static code analysis** by parsing Python source using Abstract Syntax Trees (AST) to understand code structure. It focuses on **program representation generation**, extracting structural features like class definitions, method signatures, and type annotations. This process facilitates **feature engineering from code**, providing structured metadata, including `__call__` method details via introspection, essential for AI models in code understanding or generation tasks."
nuplan-devkit/cluster_144.py,Preprocessing Text and Numerical Data,"This code implements a vectorized map representation pattern, akin to VectorNet, for autonomous driving AI. It preprocesses variable-sized map features (e.g., lanes, boundaries) into fixed-size PyTorch tensors using padding, trimming, and interpolation, generating availability masks to denote valid data. This ensures consistent input dimensions for neural networks and transforms coordinates to an ego-centric local frame, crucial for robust perception and planning models."
nuplan-devkit/cluster_148.py,Preprocessing Text and Numerical Data,"This code defines specialized data structures (`VectorMap`, `VectorSetMap`) for representing vectorized environmental maps, incorporating explicit feature encodings for elements like traffic lights and lane status. It implements a comprehensive deep learning data pipeline, featuring custom batching for variable-sized map elements, tensor conversion, and extensive geometric data augmentation (rotation, translation, scaling, flipping) to enhance model robustness. The `VectorSetMap` design, referencing VectorNet, specifically points to patterns for processing map data using graph-based or set-based neural network architectures."
nuplan-devkit/cluster_25.py,Preprocessing Text and Numerical Data,This code implements a data splitting and subsampling pattern crucial for AI model development and evaluation. It generates smaller 'mini' and 'dev' datasets by performing stratified subsampling across geographical regions within existing core data splits. This approach ensures representative subsets for faster experimentation and efficient iteration on AI models.
nuplan-devkit/cluster_41.py,Preprocessing Text and Numerical Data,"This code implements a data preprocessing pattern for managing AI-related label metadata. It transforms a raw `labelmap` (mapping integer IDs to `Label` objects) into consistently ordered dictionaries for label names and colors. This structured representation is essential for tasks like image segmentation or object detection, ensuring predictable access to class information for model training and inference."
nuplan-devkit/cluster_43.py,Preprocessing Text and Numerical Data,"This code implements core AI patterns for Lidar point cloud data handling, including robust data ingestion from various formats and essential preprocessing techniques like subsampling, proximity-based removal, and range filtering. It further provides geometric data augmentation capabilities through translation, rotation, and scaling, critical for training robust 3D AI models. Finally, the code supports advanced visualization patterns, enabling rendering of point clouds colored by height, intensity, or semantic labels, which is vital for debugging and interpreting AI model outputs."
nuplan-devkit/cluster_64.py,Preprocessing Text and Numerical Data,"The code establishes a **World Model** pattern by providing a structured, semantic representation of an autonomous driving environment, organizing map data into distinct vector and raster layers like lanes, roadblocks, and drivable areas. It further implements **Perception and Querying** patterns, offering an AI agent capabilities to perform complex spatial queries such as point-in-polygon checks, proximity searches, and nearest object distance calculations, essential for environmental understanding and navigation."
scattertext/cluster_14.py,Preprocessing Text and Numerical Data,"This code implements a robust text analysis framework, primarily utilizing Natural Language Processing (NLP) and machine learning patterns for corpus construction and analysis. It features a pipeline for building sparse term-document and metadata-document matrices from raw text, leveraging spaCy for tokenization, lemmatization, and entity extraction. The framework further includes diverse AI patterns for term scoring (e.g., scaled F-scores, posterior mean ratio, Fisher's exact test, logistic regression coefficients) and sentence-level topic modeling using TF-IDF and NMF."
scattertext/cluster_18.py,Preprocessing Text and Numerical Data,"This codebase implements a robust Natural Language Processing (NLP) pipeline, primarily utilizing spaCy for document parsing, tokenization, lemmatization, and named entity recognition to extract diverse linguistic features. These features are then structured into Term-Document Matrices, facilitating statistical term scoring, machine learning classification (e.g., logistic regression), and advanced AI patterns like word embeddings (Word2Vec) and lexical category analysis (Empath)."
scattertext/cluster_43.py,Preprocessing Text and Numerical Data,"This code implements text feature engineering patterns by extracting statistical measures such as category-specific term frequencies, document counts, and various scores (e.g., background, association) from a text corpus. It facilitates categorical text analysis by comparing term distributions across defined categories, enabling the identification and interactive exploration of distinguishing terms and their characteristics, integrating both text and non-text features."
scattertext/cluster_53.py,Preprocessing Text and Numerical Data,"This code implements two distinct AI patterns for **term significance analysis** using **log-odds ratios**, a common technique in Natural Language Processing for feature selection. It demonstrates different **statistical smoothing methods**, specifically ""add-one"" smoothing and an ""uninformative Dirichlet prior,"" to robustly calculate **z-scores and p-values** from word counts, addressing data sparsity and incorporating prior knowledge."
scattertext/cluster_6.py,Preprocessing Text and Numerical Data,"This code implements core Natural Language Processing (NLP) patterns for text corpus management, focusing on building and manipulating Term-Document Matrices (TDM) from diverse sources, including tokenization, stoplisting, and metadata integration. It features a suite of term weighting and characteristicness scoring algorithms, such as log-odds with uninformative priors, F-scores, and logistic regression coefficients, to identify distinguishing vocabulary across categories. Additionally, it provides structured frameworks like Semiotic Square and Four Square Axes for advanced semantic and discourse analysis."
scattertext/cluster_63.py,Preprocessing Text and Numerical Data,"This code implements core Natural Language Processing (NLP) patterns for text representation and feature engineering, primarily through the construction and manipulation of Term-Document Matrices. It incorporates diverse statistical methods for term weighting and ranking, such as F-scores, posterior mean ratios, and Fisher's exact test, alongside feature selection techniques like stop-word removal and automated term reduction. The patterns extend to supervised learning, utilizing Logistic Regression for document classification and extracting term importance."
scattertext/cluster_78.py,Preprocessing Text and Numerical Data,"This code demonstrates patterns for custom Natural Language Processing (NLP) preprocessing, including rule-based tokenization, basic part-of-speech tagging, and sentence segmentation. It further illustrates feature engineering for NLP tasks, extracting unigrams, bigrams, and lemmas, with support for entity and tag-based censoring. A key pattern is the integration of pre-trained transformer model tokenizers, adapting their output to a custom document structure while handling text decoding and further segmentation."
surpriver/cluster_0.py,Preprocessing Text and Numerical Data,"This code implements a robust **Feature Engineering** pipeline for financial time series, leveraging a `TAEngine` to derive technical indicators and extract features from raw market data. It incorporates **Data Preprocessing and Filtering** mechanisms, such as volume and volatility thresholds, to ensure data quality and consistency. The system is designed for **Dataset Preparation for Time Series Prediction**, handling historical and future price data for training and evaluating predictive AI models."
synthetic-data-generator/cluster_34.py,Preprocessing Text and Numerical Data,"The code implements patterns for automated data preprocessing and privacy-preserving transformations. The `HyperTransformer` orchestrates dynamic selection and chaining of transformers based on data types, enabling flexible feature engineering pipelines. Specialized transformers like `LabelEncoder` handle categorical encoding, while `AnonymizedFaker` and `PseudoAnonymizedFaker` provide distinct approaches for data anonymization, either by random replacement or traceable mapping, to prepare sensitive data for AI applications."
synthetic-data-generator/cluster_59.py,Preprocessing Text and Numerical Data,"This code implements a Conditional Tabular Generative Adversarial Network (CTGAN) pattern for synthetic data generation. It involves training a generator and a discriminator, with configurable hyperparameters for network dimensions, learning rates, and training epochs, to learn the distribution of input tabular data. The pattern supports both continuous and discrete features, enabling the creation of new data samples, including conditional sampling based on specific column values."
tf.fashionAI/cluster_25.py,Preprocessing Text and Numerical Data,"This code implements a comprehensive **data augmentation pipeline** for computer vision, featuring random image rotations, color distortions (brightness, saturation, hue, contrast), and distorted bounding box cropping. It specifically targets **keypoint detection** by transforming keypoint coordinates during augmentation and generating Gaussian heatmaps as model targets, alongside standard **data normalization** via mean image subtraction."
torchio/cluster_13.py,Preprocessing Text and Numerical Data,"This code implements a **data preprocessing AI pattern** for medical imaging, specifically histogram standardization, to normalize image intensity values. It follows a **two-phase learning approach**: a `train` method statistically learns average histogram landmarks from a dataset, which are then applied during inference by `apply_normalization` to ensure consistent input representations for downstream AI models. This pattern is crucial for enhancing feature robustness in AI applications dealing with varied medical image acquisitions."
traceroot/cluster_25.py,Preprocessing Text and Numerical Data,"This code exemplifies data preprocessing and feature engineering patterns for hierarchical tracing data. It utilizes recursive traversal and aggregation to summarize log metrics from nested spans up to trace objects. Furthermore, it performs statistical feature derivation by categorizing trace durations into percentiles, preparing structured data for potential AI-driven analysis or anomaly detection."
AIlice/cluster_37.py,none,"This code implements a rule-based interpreter that leverages regular expressions to define and recognize various AI patterns, including data types, variable declarations, function calls, and object expressions. It dynamically registers these patterns and maps them to specific evaluation functions, enabling the processing and execution of a domain-specific language. This architecture facilitates symbolic processing by parsing structured commands and executing corresponding actions within its environment."
ChatSim/cluster_12.py,none,"This code implements AI patterns for **pose interpolation**, generating intermediate camera poses (rotations and translations) represented by Dual Quaternions. It offers two distinct methods: **linear interpolation** for direct transitions and **Hermite spline interpolation** for creating smoother, more complex trajectories between existing poses. This enables the synthesis of new viewpoints or continuous motion paths from discrete image data."
evadb/cluster_54.py,none,"The code outlines a rule-based query optimizer, categorizing rules into rewrite and implementation stages, specifically tailored for AI/ML workloads. It features specialized AI patterns like embedding filters and samples directly into data retrieval, optimizing similarity searches via vector index scans, and transforming object extraction operations. The system further supports distributed execution for AI tasks, evident from conditional physical plan transformations leveraging frameworks like Ray."
3DOD_thesis/cluster_1.py,none,"This code implements a 3D object pose estimation pipeline, where a `BoxRegressor` refines 3D bounding box parameters (size, position, orientation) using least-squares optimization. It minimizes residuals between projected 3D keypoints and predicted 2D keypoints, incorporating regularization for size and distance. Furthermore, PyTorch `Dataset` classes prepare and augment image data, applying random 2D bounding box transformations, horizontal flipping, and normalization of image pixels and 3D labels for robust deep learning model training and evaluation."
3DOD_thesis/cluster_4.py,none,"This code defines a PyTorch Dataset for 3D object detection, specifically preparing augmented frustum point clouds from the KITTI dataset for models like Frustum PointNet. It performs extensive data augmentation, including 2D bounding box jittering, frustum alignment, random Z-shift, horizontal flipping, and object-centric rotation of ground truth points. The dataset outputs sampled point clouds, instance segmentation masks, and multi-component 3D bounding box labels (center, dimensions, orientation bin/residual, corners) for robust training."
3DOD_thesis/cluster_5.py,none,"This code implements a specialized data loading and preprocessing pipeline for 3D object detection, following the Frustum PointNet pattern. It integrates multi-modal sensor data by projecting LiDAR point clouds into camera coordinates and extracting 3D frustums based on 2D image bounding boxes. The pipeline incorporates extensive data augmentation, including 2D bounding box jittering, point cloud Z-shifting, and horizontal flipping, alongside coordinate system normalization and orientation binning for robust 3D pose estimation and instance segmentation."
AIlice/cluster_14.py,none,"This code implements a distributed communication pattern for AI components, enabling remote instantiation and invocation of AI models. It specifically supports streaming results from remote AI computations through generator proxies. This architecture facilitates dynamic integration of diverse AI services by allowing clients to adapt their interfaces based on server-provided API metadata."
AIlice/cluster_15.py,none,"This code implements a **Client Pooling pattern** to efficiently manage connections to various modular AI services, facilitating resource optimization in a distributed AI system. It supports **on-demand client creation** and secure inter-service communication, enabling a flexible architecture where AI modules can be dynamically accessed. Explicit lifecycle management, including graceful client destruction and potential deregistration, ensures the stable operation of interconnected AI components."
Automatic_Speech_Recognition/cluster_9.py,none,"This code implements a Capsule Network architecture, featuring a `CapsuleLayer` that transforms inputs into vector-based capsules. It utilizes the dynamic routing algorithm, which iteratively refines coupling coefficients between capsules, and a `squashing` function to normalize the length of capsule output vectors. The `CapsuleLayer` supports both convolutional and fully connected transformations to generate these capsules, demonstrating flexible integration into neural network architectures."
CAAFE/cluster_3.py,none,"This code primarily implements sophisticated data visualization patterns, specifically generating highly customizable stripplots and pointplots for statistical data representation. It focuses on presenting data distributions and central tendencies across categories using `seaborn` and `matplotlib`. While these visualization patterns are critical for exploratory data analysis and interpreting results in AI/ML workflows, the code itself does not contain intrinsic AI algorithms or machine learning models."
ChatSim/cluster_143.py,none,"This code showcases a hybrid AI system that integrates deep learning with large language models for autonomous agent control. It features a `LearnableSpatialTransformWrapper` for adaptive data augmentation or equivariant processing, where rotation angles are learned parameters. A `MotionAgent` leverages a GPT-4 LLM through extensive prompt engineering to interpret natural language commands for scene-dependent and independent vehicle placement and motion planning within a simulated environment."
ChatSim/cluster_41.py,none,"This code implements the Learned Perceptual Image Patch Similarity (LPIPS) as a neural network module. It leverages deep feature extraction from pre-trained backbone networks (e.g., AlexNet, VGG) to compute a perceptually-aligned distance between input images. The core AI pattern involves applying learned linear transformations to the squared differences of these multi-scale features, effectively creating a learned perceptual metric."
ChatSim/cluster_63.py,none,"This code implements a scalable data pipeline for AI, utilizing `webdataset` for efficient sharded data loading and writing. It employs parallel input/output streams, in-memory shuffling, and category-based filtering to prepare and curate large datasets for deep learning model training."
CyberScraper-2077/cluster_5.py,none,"This code implements an AI pattern for **structured output parsing and interpretation**, specifically designed to process AI-generated content that embeds tabular data (CSV or Excel) within markdown code blocks. It automatically extracts this structured data, formats it into interactive dataframes, and provides user interface elements for visualization, download, and potential integration with external services like Google Sheets. This enables the AI to effectively communicate complex data in a machine-readable and user-friendly format."
DLTA-AI/cluster_2.py,none,"This code demonstrates patterns crucial for maintaining data integrity and facilitating human-in-the-loop data curation in AI-related applications. The `check_duplicates_editLabel` function implements a data validation pattern, ensuring unique identifiers for entities (shapes) across frames, which is vital for consistent object tracking or annotation datasets. Furthermore, the `PopUp` function exemplifies an interactive data correction pattern, engaging the user to resolve duplicate ID conflicts and thereby improving the quality of input data for subsequent AI model training or analysis."
DLTA-AI/cluster_26.py,none,"This code implements a factory pattern for generating diverse sets of anchor boxes, a core AI pattern in object detection. It showcases distinct anchor generation strategies tailored for ""Standard,"" ""SSD,"" and ""YOLO"" detectors, each defining multi-level anchors with specific configurations like scales, ratios, strides, or pre-defined base sizes. These generators are crucial for proposing candidate object locations and scales across various feature map resolutions."
DLTA-AI/cluster_47.py,none,"The code implements the Varifocal Loss, an AI pattern specifically tailored for object detection to mitigate class imbalance and improve learning from dense predictions. It achieves this by dynamically weighting loss contributions using IoU targets and modulating factors (`alpha`, `gamma`). Furthermore, the implementation follows a modular design for registering custom loss functions and utilizes JIT compilation for performance optimization of the core loss calculation."
EvoAgentX/cluster_12.py,none,"This code implements patterns for static code analysis, including parsing, abstract syntax tree traversal, and dependency graph construction, which are crucial for understanding and manipulating code. It further provides a robust framework for evaluating AI-generated code solutions against unit tests, incorporating mechanisms for sanitization, execution, and performance metrics like `pass@k` to assess model efficacy in code generation or program synthesis tasks."
EvoAgentX/cluster_17.py,none,"This code implements a **tool-based AI interaction pattern**, providing specialized classes to encapsulate interactions with OpenAI's image generation, editing, and multimodal analysis APIs. It employs robust **input validation and pre-processing patterns**, dynamically checking model-specific parameters and adapting image formats (e.g., converting to RGBA or base64 data URLs) to ensure compatibility and reliability with diverse AI models."
FinceptTerminal/cluster_27.py,none,"This code implements a classical **portfolio optimization** pattern, constructing an efficient frontier to identify optimal asset allocations. It heavily utilizes **numerical optimization algorithms**, specifically constrained minimization (e.g., SLSQP via `scipy.optimize.minimize`), to find portfolios that minimize variance, maximize Sharpe ratio, or meet specific return targets. This approach solves a prescriptive analytics problem by determining optimal investment strategies based on expected returns and covariance."
FuxiCTR/cluster_40.py,none,"This code implements a flexible pattern for integrating pre-trained embeddings with randomly initialized ID embeddings, offering fusion strategies like initialization, summation, or concatenation. It incorporates mechanisms to freeze or fine-tune pre-trained weights, project combined embeddings to a consistent dimension, and handle out-of-vocabulary tokens through masking."
GoBigger/cluster_7.py,none,"The code defines an agent-based simulation environment, featuring `CloneBall` agents with sophisticated, state-dependent behaviors for physics-driven movement, resource acquisition (eating), and strategic actions like splitting and ejecting spores. It models complex interactions including rigid body collisions between agents, reactive environmental elements (`ThornsBall`s) that move upon consuming other entities, and continuous score decay, forming a dynamic system suitable for training reinforcement learning agents."
HuixiangDou/cluster_31.py,none,"This code implements a robust AI interaction management system, leveraging Redis for caching chat queries and their AI-generated responses, alongside orchestrating an AI inference task queue. It incorporates patterns for tracking AI system usage, such as total inferences, active users, and agent-specific activity. Additionally, it provides a mechanism for collecting feedback on AI responses, facilitating operational monitoring and potential model refinement."
HuixiangDou/cluster_42.py,none,"The code implements an asynchronous AI chat service that processes multimodal inputs (text and images) by submitting tasks to an AI backend and managing interaction state via a caching layer. It integrates agent-based AI interactions, exemplified by the Lark platform, which parses messages, routes them to specific AI models using a `feature_store_id`, and manages response delivery. This includes patterns for AI usage monitoring and feedback collection to support continuous improvement."
MassGen/cluster_31.py,none,"The code demonstrates a rule-based decision system for file operation policies. It utilizes pattern matching on file and directory names (e.g., `__pycache__`, `.pyc`, `.pytest_cache`) to classify them. This enables heuristic-driven exemptions from a general ""read before delete"" rule, representing a simple form of symbolic AI for intelligent file management."
WebShop/cluster_2.py,none,"This code defines a simulated e-commerce environment, `SimServer`, designed for goal-oriented AI agents. It initializes user sessions with specific, often randomized, shopping goals and tracks agent interactions through search, navigation, and product selection. The environment provides a reward signal upon goal completion, facilitating reinforcement learning or other agent-based evaluation."
evadb/cluster_103.py,none,"This `CatalogManager` centralizes the management of AI/ML operational components, acting as a comprehensive registry for functions, their I/O, metadata, and associated costs. It specifically handles AI-centric patterns like vector indexing for efficient data retrieval and function result caching for performance optimization. Furthermore, it orchestrates AI/ML workflows by managing job schedules, execution history, and diverse multimedia data tables."
evadb/cluster_12.py,none,"This code defines a SQL-like parser and API for an AI database, showcasing patterns for integrating and querying AI models. It supports defining custom AI functions (e.g., `Yolo`, `FeatureExtractor`, `FaceDetector`, `ChatGPT`) that operate on specialized `NDARRAY` data types for tasks like object detection, feature extraction, forecasting, and LLM interaction. Key AI patterns include vector indexing (FAISS, QDRANT) for similarity search, lateral joins to apply AI functions row-wise, and explicit support for AI model types and document chunking for NLP."
evadb/cluster_20.py,none,"This code does not exhibit any discernible AI patterns. It primarily functions as a utility for generating UML class diagrams from object-relational mapper metadata, focusing on graph construction, label formatting, and relationship visualization rather than machine learning, data analysis, or intelligent decision-making."
evadb/cluster_60.py,none,"The code implements a rule-based intelligent agent pattern for job scheduling, where a `JobScheduler` makes deterministic decisions about task execution. It utilizes explicit temporal rules to calculate sleep durations and update future run schedules, effectively orchestrating automated tasks based on predefined conditions and recurrence intervals. This pattern focuses on stateful decision-making through predefined logic rather than adaptive learning."
evadb/cluster_68.py,none,"The code implements a **Visitor pattern** for traversing an Abstract Syntax Tree (AST), with methods like `select_elements`, `table_source`, and `query_specification` processing specific grammar rules. It employs **recursive descent parsing/interpretation** to systematically extract and transform complex SQL-like query elements, including various join types, subqueries, and `UNION` operations, into a structured internal representation. This pattern is essential for building robust query parsers, a foundational component for data processing and understanding in many AI-driven applications."
factorio-learning-environment/cluster_10.py,none,"This code implements AI patterns for **synthetic spatial data generation** and **prioritized data sampling**. It creates diverse 2D position datasets using geometric patterns like grids and various spirals, and organizes them with priority given to samples closer to the origin. This approach facilitates efficient exploration and focused learning for AI agents operating in spatial environments."
factorio-learning-environment/cluster_17.py,none,"This code implements an AI-assisted operational monitoring system that utilizes rule-based logic and thresholds to detect anomalies in Factorio production metrics, such as significant changes in resource flows or supply deficits. Critical warnings and detected operational issues are then actively relayed in real-time to an external AI agent, ""Claude Code,"" via a tmux session, establishing a communication channel for AI-driven analysis or intervention."
factorio-learning-environment/cluster_22.py,none,"This code implements patterns for interacting with Large Language Models (LLMs), including robust parsing of LLM responses to extract token usage and programmatically extract generated code. It further incorporates prompt engineering techniques for conversational AI, such as filtering and merging messages to optimize context and reduce token consumption."
factorio-learning-environment/cluster_80.py,none,"This code implements a distributed Monte Carlo Tree Search (MCTS) pattern, orchestrating multiple parallel MCTS instances. Each instance operates within a dedicated group, utilizing `FactorioInstance`s and an `Evaluator` to assess states and guide the search. The design leverages an `APIFactory` to integrate language models, enabling the MCTS to explore and optimize program generation or state discovery concurrently."
mcp-agent/cluster_1.py,none,"This code primarily demonstrates robust unit testing patterns for a CLI application managing server-side application workflows and configurations. While it extensively uses mocking and patching to simulate server interactions and test various client scenarios, it does not exhibit any explicit AI patterns such as model training, inference, or specialized data processing techniques."
mcp-agent/cluster_2.py,none,"This code analyzes log events from an AI system, specifically identifying patterns of agent behavior. It tracks distinct agents (referred to as MCPs) and their actions, such as engaging in conversational turns (`CHATTING`) and utilizing external functionalities (`CALLING_TOOL`). The system provides observability into these agent-based interactions and tool-use patterns by parsing raw events into structured progress events for summary and detailed display."
mcp-agent/cluster_25.py,none,"This code implements patterns for **LLM token usage monitoring and observability**. It features a robust token extraction mechanism (`_extract_tokens`) that parses various log entry formats to quantify LLM interactions. The `logs` command-line interface further enables filtering, sorting (including by token count), and real-time tailing of these AI-specific operational logs, facilitating performance and cost analysis of AI applications."
mcp-agent/cluster_52.py,none,"This code demonstrates patterns for **contextual resource resolution** and **heterogeneous data handling**, crucial for AI applications. The `find_resource_file` function implements a pattern for dynamically locating assets (e.g., model weights, configuration files) relative to a set of ""prompt files,"" mirroring how AI systems often resolve dependencies based on a primary input's context. Furthermore, `load_resource_content` robustly processes diverse data types (text, binary via base64 encoding) based on MIME type, a critical pattern for multimodal AI pipelines that handle various input formats."
mcp-agent/cluster_94.py,none,"This code primarily demonstrates robust unit testing patterns for a CLI application's status command, focusing on isolating dependencies through extensive mocking and patching. It validates various scenarios, including successful data retrieval and error handling for missing or invalid inputs, ensuring the reliability of status reporting. No specific AI patterns, such as model inference, data preprocessing for AI, or AI-specific deployment strategies, are represented within this code."
monkey-net/cluster_3.py,none,"This code implements a keypoint-driven video generation framework. It utilizes a `kp_detector` for extracting motion-relevant keypoints and a `PredictionModule` for sequence prediction, forecasting future keypoints from initial frames. A `generator` then synthesizes new video sequences by combining a static appearance with these keypoints, enabling tasks like video reconstruction and motion transfer."
nuplan-devkit/cluster_118.py,none,"This code demonstrates patterns for simulating and logging critical data for autonomous systems, including `EgoState`, `SimulationHistory`, and `TrafficLightStatusData`, which capture an AI agent's state, trajectory, and environmental context. It employs property-based testing to validate the serialization and integrity of this complex simulation data, ensuring reliable persistence of observations, world states, and agent actions for AI model development and evaluation. The use of mocks for `AbstractSimulationTimeController`, `AbstractObservation`, and `AbstractEgoController` further exemplifies a modular approach to testing interactions between core AI system components."
nuplan-devkit/cluster_151.py,none,"This code implements a pattern for visualizing AI agent states and behaviors by processing `SimulationHistory` data, specifically `DetectionsTracks` of `tracked_objects`. It extracts key features like position, velocity, and heading, and employs a tracking mechanism to assign consistent IDs to agents across frames. This processed data dynamically renders interactive plots, enabling real-time observation of AI agent dynamics."
nuplan-devkit/cluster_154.py,none,"This code implements a pattern for visualizing and analyzing results from AI/ML experiments, specifically through a dashboard tab interface. It manages `ExperimentFileData` and `MetricStatisticDataFrame` objects, enabling scenario-based filtering and the display of performance metrics. The class provides utilities for dynamic plot generation and data selection, crucial for interpreting AI model behavior across different simulation scenarios."
nuplan-devkit/cluster_2.py,none,"This codebase implements AI patterns for **trajectory generation and control**, leveraging interpolation for smooth paths and model-predictive control (iLQR, LQR) for robust tracking. It further integrates **behavioral planning** (Intelligent Driver Model) and **machine learning models** for agent prediction and planning, supported by dedicated **feature engineering** to process diverse sensor and map data."
nuplan-devkit/cluster_20.py,none,"This code implements a specialized database (`NuPlanDB`) pattern for managing multimodal sensor and ground truth data, such as camera, lidar, ego-pose, object tracks, and scenario tags, collected from expert-driven logs. It functions as a dedicated data source or feature store, providing structured access to diverse inputs crucial for training and evaluating autonomous driving AI models. This pattern supports tasks like perception, prediction, and potentially imitation learning by organizing complex real-world driving scenarios."
nuplan-devkit/cluster_3.py,none,"This codebase implements diverse AI patterns for autonomous driving, featuring a Linear Quadratic Regulator (LQR) for precise vehicle control and an Intelligent Driver Model (IDM) for rule-based behavioral planning. It further integrates advanced machine learning planners, such as VectorNet and RasterNet, which are supported by extensive data querying for scenario generation and specialized feature engineering from sensor and map data. The system also includes robust visualization tools to analyze these AI agents' performance within simulated scenarios."
nuplan-devkit/cluster_35.py,none,"This code tests a `Category` class, likely representing semantic labels or object classes within an AI dataset like NuPlan. It specifically verifies the assignment of default colors for these categories, a common AI pattern for visualizing model outputs such such as detections or segmentations. Furthermore, it ensures proper interaction with a database ORM for managing category metadata, crucial for large-scale AI data management."
nuplan-devkit/cluster_54.py,none,"This codebase implements robust AI patterns for distributed workload execution and cloud-native data management, leveraging worker pools and explicit synchronization to process scenarios and metrics across multiple nodes, with extensive S3 path handling for AI artifacts. It further incorporates MLOps patterns for comprehensive experiment tracking and evaluation, including metric computation, aggregation, visualization, and performance profiling. This architecture facilitates scalable scenario-based simulation and testing, crucial for developing and validating AI systems."
optillm/cluster_40.py,none,"This code exemplifies a pattern of **model steering** applied to large causal language models, where a `steering_dataset` and `target_layer` are used to influence the model's internal representations. It further implements **controlled generation** by defining explicit `pattern_strengths` (e.g., 'self_correction', 'exploration') within an `AutoThinkProcessor` to guide the model's ""thinking process"" and output characteristics. This allows for fine-grained control over the generated response's style and content."
pybroker/cluster_21.py,none,"The code primarily employs the **bootstrap method**, a statistical resampling AI pattern, to rigorously estimate confidence intervals for key financial performance metrics like maximum drawdown, Sharpe ratio, and profit factor. This approach quantifies the uncertainty and robustness of evaluations by generating multiple random samples from the observed data, crucial for assessing AI-driven quantitative strategies. Further, the use of Numba's `@njit` decorator optimizes these data-intensive computations for high performance."
semantic-router/cluster_14.py,none,"This code implements a unified pattern for generating AI embeddings by leveraging the LiteLLM library, with `LiteLLMEncoder` serving as a base for provider-specific adapters like Nim, Voyage, and Cohere. It supports both synchronous and asynchronous embedding generation, explicitly differentiating between query and document input types for asymmetric embedding models via `litellm`'s `input_type` parameter to optimize for retrieval tasks."
semantic-router/cluster_40.py,none,"This code demonstrates AI patterns for semantic routing, integrating an `encoder` (embedding model) with a `vector index` to classify inputs and direct them to predefined routes. It showcases patterns for dynamic route management, including adding, deleting, and querying routes, and incorporates a `HybridRouter` pattern for combining different scoring mechanisms. Furthermore, the system includes patterns for evaluating and adapting the routing layer's performance based on provided data."
synthetic-data-generator/cluster_62.py,none,"This code implements a **Gaussian Copula-based synthetic data generation model**, leveraging `copulas.multivariate.GaussianMultivariate` to capture the joint distribution of features. It performs **univariate distribution modeling** for individual columns, allowing selection from various statistical distributions (e.g., Gaussian, Beta, Gamma). The model incorporates **statistical parameter estimation** during fitting, including robust mechanisms like correlation matrix regularization to ensure the mathematical validity and stability of the generative process."
synthetic-data-generator/cluster_77.py,none,"This code primarily demonstrates synthetic data generation patterns, crucial for creating artificial datasets for AI development and testing. It includes functions for generating structured, randomized credit codes using character selection and a comprehensive fixture that populates a personal information DataFrame with realistic-looking data via the Faker library. This approach enables the creation of diverse test data for AI models without relying on sensitive real-world information."
tabby/cluster_0.py,none,"This code implements a pattern for deploying and serving AI models, specifically large language models (LLMs) like Tabby, by launching a dedicated `serve` process configured with specific `MODEL_ID`s. It leverages GPU acceleration and parallelism to optimize inference performance and handle concurrent requests efficiently. The AI service is exposed via an ASGI application, incorporating a readiness probe to ensure the model server is fully operational before accepting traffic."
tabby/cluster_2.py,none,"This code implements a client-side API interaction pattern, crucial for integrating with AI services. It facilitates structured data ingestion through `LogEventRequest`, a common pattern for feeding telemetry or training data into AI systems. Furthermore, it defines the consumption of `SearchResponse` objects, representing the client-side interface for an AI-powered search capability where the intelligence resides remotely."
traceroot/cluster_22.py,none,"This code implements a recursive aggregation pattern for hierarchical data, specifically accumulating log counts across a tree-like structure of 'spans.' In AI contexts, this pattern is crucial for **observability and monitoring of complex AI pipelines or model inference traces**. It enables the aggregation of operational metrics from individual components up to a higher-level view, facilitating debugging and performance analysis of multi-stage AI systems."
transfuser/cluster_19.py,none,"This code demonstrates an agent-environment interaction pattern within the CARLA autonomous driving simulator. It explicitly features an ""autopilot"" mode, representing an AI agent responsible for autonomous vehicle control, which can operate alongside or instead of manual input. The setup facilitates simulation-based development and observation of AI agent behavior in a controlled environment."
transfuser/cluster_21.py,none,This code implements a sophisticated **behavior tree pattern** using `py_trees` to orchestrate complex AI agent actions and environmental interactions within simulated driving scenarios. It features a **scenario-based testing framework** that dynamically constructs event-driven behaviors and performance evaluation criteria from both predefined routes and OpenSCENARIO XML definitions.
transfuser/cluster_23.py,none,"The `CarlaDataProvider` establishes a centralized **World Model** and **Perception System** for AI agents within the CARLA simulation, continuously buffering actor states like velocity and location. It further functions as an **Agent Factory** and **Environment Interface**, enabling the dynamic creation and management of multiple agents and providing control over environmental elements such as traffic lights, crucial for autonomous navigation AI. This comprehensive data and control layer supports robust **Scenario Generation** for diverse AI training and evaluation setups."
transfuser/cluster_30.py,none,"This codebase implements an autonomous driving agent leveraging classical AI patterns for navigation and control. It utilizes PID controllers for precise vehicle dynamics, predictive models for future state estimation of ego and other actors, and robust hazard detection through bounding box intersection and trajectory extrapolation for collision avoidance. Furthermore, the system includes scenario generation capabilities to create diverse driving situations like junctions and curved roads for testing."
AIlice/cluster_19.py,none,"The code implements an AI agent's core interaction patterns, featuring an `AInterpreter` that parses and executes a domain-specific language for variable management, object instantiation, and function calls using dynamically generated regex patterns. It integrates multimodal conversational processing (`AConversations`) to automatically extract and manage code snippets and media references from user input, making them accessible within the interpreter's environment. A custom `AJSONDecoder` and associated serialization functions further support the structured persistence and deserialization of these AI-specific data types, enabling robust state management."
mindpalace/cluster_2.py,none,"This code implements a visualization pattern for AI-generated relational data, specifically transforming structured relationships into Mermaid flowcharts. It processes inputs likely derived from AI tasks such as information extraction or knowledge graph construction, where AI models identify entities and their connections. This pattern facilitates the interpretation and debugging of complex relationships discovered by AI, making AI-derived insights visually accessible."
AIlice/cluster_16.py,Model Abstraction,"The code establishes a multi-model AI abstraction layer, providing a unified `Generate` interface for diverse Large Language Models, encompassing both API-based services (OpenAI, Anthropic, Mistral) and local Hugging Face Causal LMs. A consistent pattern emerges across these models, featuring streaming token generation, real-time sentence-level output processing, and robust token consumption tracking, alongside support for advanced configurations like quantization and PEFT for local models."
AIlice/cluster_2.py,Model Abstraction,"This code implements a configurable AI system that supports dynamic parameterization of Large Language Models (LLMs) for inference, allowing customization of model ID, quantization, memory, and prompt engineering, with options for agent-specific model usage. It also includes a comprehensive LLM fine-tuning pattern, leveraging 4-bit quantization, PEFT (LoRA), and custom data tokenization, while integrating speech-to-text and text-to-speech functionalities for interactive AI agents."
AIlice/cluster_6.py,Model Abstraction,"This code implements a **Session-Based AI Agent Management** pattern, where each `TaskSession` encapsulates an `AProcessor` that orchestrates interactions with an `ALLMPool` for model access and an `APromptsManager` for prompt engineering. It utilizes a **User-Specific AI Configuration** pattern within `UserContext` to manage individual user settings for agent models, providers, and parameters. Furthermore, the system provides **Real-time AI Output Streaming** via server-sent events for dynamic, interactive responses from the AI agent."
CyberScraper-2077/cluster_7.py,Model Abstraction,"This code implements a multi-model AI system, enabling users to select and switch between various commercial Large Language Models (LLMs) such as GPT and Gemini, alongside locally hosted Ollama models. It features a conversational AI agent (`web_scraper_chat`) that processes user prompts, specifically designed for web scraping by interpreting URLs and subsequently answering questions about the extracted data. This demonstrates a pattern of dynamic LLM selection and an AI agent performing specialized tool-use for data interaction."
EvoAgentX/cluster_52.py,Model Abstraction,"This code implements a **unified Large Language Model (LLM) interface pattern**, abstracting interactions with multiple AI providers (e.g., OpenAI, Azure, Anthropic) through a single `LiteLLM` class. It employs a **model registration pattern** for dynamic extensibility and uses **configuration-driven initialization** to manage provider-specific API keys and endpoints. This design facilitates robust and efficient AI service integration, incorporating retry mechanisms and asynchronous generation capabilities."
EvoAgentX/cluster_55.py,Model Abstraction,"This code defines an **LLM Abstraction Layer** through the `BaseLLM` abstract class, establishing a standardized interface for interacting with various Large Language Models. It incorporates essential AI patterns such as **prompt formatting** to adapt inputs for different LLM APIs, **multimodal input handling** for processing diverse content types like images, and **structured output parsing** to transform raw LLM responses into usable data. This design facilitates interchangeable LLM backends and streamlines complex AI interactions, including synchronous and asynchronous generation."
GPT_Vuln-analyzer/cluster_10.py,Model Abstraction,"This code exemplifies a pattern of leveraging multiple large language models (Bard, Llama, GPT) for specialized NMAP scan analysis. It employs detailed prompt engineering to instruct the AI models to act as pentesters, extracting critical security information and vulnerabilities from scan data. The system enforces structured output formats, utilizing regex to parse and standardize the AI-generated reports into predefined categories like critical score, open ports, and found CVEs."
GoBigger/cluster_19.py,Model Abstraction,"This code implements a **modular neural network layer construction pattern**, providing configurable building blocks for fully-connected and 2D convolutional layers. It utilizes **factory patterns** (`build_normalization`, `build_activation`) to dynamically select and instantiate various normalization techniques and activation functions. The presence of `fc_block` vs `fc_block2` and `conv2d_block` vs `conv2d_block2` further demonstrates the exploration of **architectural micro-patterns** by varying the order of operations within these fundamental layers."
MassGen/cluster_5.py,Model Abstraction,"This codebase primarily implements robust **tool-use and function-calling patterns** across various LLM backends, enabling models to dynamically invoke external capabilities like web search, code interpreters, and custom services through streaming interfaces. It further integrates **Retrieval Augmented Generation (RAG)** via file uploads and vector stores, alongside sophisticated **multi-agent coordination protocols** that leverage structured output and incorporate resilience patterns such as circuit breakers and retry logic for robust tool execution. The system also supports streaming the LLM's internal reasoning process, enhancing transparency in complex AI workflows."
RealtimeVoiceChat/cluster_1.py,Model Abstraction,"This code establishes a unified interface for interacting with diverse LLM backends (Ollama, OpenAI, LMStudio), employing an **Adapter pattern** to provide a consistent **streaming generation** experience. It integrates robust AI operational patterns, including **lazy initialization with connection resilience** (e.g., `ollama ps` fallback) and **prewarming** for optimized startup. Additionally, it supports **request cancellation** for active streams and **inference time measurement** for performance analysis."
VectorizedMultiAgentSimulator/cluster_16.py,Model Abstraction,"This code demonstrates a pattern of **scenario-based reference path generation**, where predefined topological structures like intersections, merge-ins, merge-outs, and loops are explicitly defined and used to construct specific driving trajectories. It performs detailed **geometric map processing** by concatenating lanelet boundaries and deriving continuous center lines with associated yaw and vector information. This foundational geometric representation and library of pre-computed paths are crucial for supporting AI-driven navigation, behavior planning, and trajectory execution in autonomous systems."
evolving-agents/cluster_4.py,Model Abstraction,"This code implements an **Agent Factory pattern** for creating and managing AI agents, utilizing a **provider abstraction layer** to support various AI frameworks like 'beeai' and 'openai-agents'. It facilitates **dynamic loading and execution of agent code snippets** from a library, allowing for flexible agent instantiation and their integration as **executable tools** within a larger system. This includes adapting tools for specific AI frameworks and robustly extracting identifiers from diverse AI model responses."
factorio-learning-environment/cluster_32.py,Model Abstraction,"This code demonstrates an AI pattern of **multi-model large language model (LLM) integration** for **code generation and refactoring**, leveraging services like OpenAI, Anthropic, and DeepSeek to transform Factorio blueprint implementations. It employs **prompt engineering** to guide these LLMs in generating refactored Python code that adheres to specific structural and functional requirements. A critical **AI output verification** pattern is present, where generated code is executed and validated against the original blueprint's entity placement, with iterative attempts to achieve successful refactoring."
jodie/cluster_1.py,Model Abstraction,"This code implements a data loading and feature engineering pattern for sequential interaction data, commonly used in dynamic graph learning or temporal recommendation systems. It extracts crucial temporal features, including time differences between consecutive user and item interactions, and the user's immediate previous item. This structured data, along with interaction-specific features and state labels, prepares the input for models that learn from evolving interaction sequences and temporal dynamics."
mcp-agent/cluster_67.py,Model Abstraction,"This code implements an **LLM Provider Abstraction** pattern, dynamically loading specific LLM backend classes like OpenAI or Anthropic based on a given provider name. It incorporates a **Model Selection and Routing** mechanism to choose appropriate models using explicit IDs or `ModelPreferences`, and utilizes an **LLM Factory** pattern to encapsulate the creation and configuration of these augmented LLM instances. This design facilitates interchangeable LLM backends and flexible model management."
optillm/cluster_18.py,Model Abstraction,"This code implements a multi-provider AI client orchestration pattern, dynamically instantiating clients for services like OptiLLM, Cerebras, OpenAI, and Azure OpenAI based on environment variables, with LiteLLMWrapper serving as a unified fallback. It also establishes an API gateway pattern by proxying a standard `/v1/models` endpoint, abstracting model listing across these diverse AI providers. A strong emphasis is placed on configurable and secure AI API interaction, allowing flexible SSL verification for the underlying HTTP clients used by these services."
optillm/cluster_19.py,Model Abstraction,"This code implements an LLM proxy pattern, handling input normalization to ensure message content compatibility across various models and parsing tagged conversations for structured interaction. It features a dynamic strategy pattern, `parse_combined_approach`, which selects between a direct pass-through approach (`none`) and orchestrates multi-approach operations (e.g., 'AND', 'OR') for potential ensemble or multi-model inference."
semantic-router/cluster_26.py,Model Abstraction,"This code demonstrates patterns for integrating with and testing **external AI embedding models** from providers like OpenAI, Azure OpenAI, and Cohere. It implements robust **API integration** by incorporating comprehensive retry logic for transient errors and supporting both synchronous and **asynchronous AI operations**. Key patterns also include secure API key management and handling input text length constraints (truncation) for these embedding services."
AIlice/cluster_10.py,Model Abstraction,"This code implements a robust AI configuration management pattern, allowing users to define and dynamically update settings for various AI models and providers (e.g., OpenAI, Groq, Anthropic) including agent-specific configurations, temperature, and context window ratios. It further employs a session management pattern to encapsulate individual AI interaction flows, providing lifecycle control (create, load, delete) and maintaining conversational history. A state machine governs the user context's readiness and release, ensuring structured and personalized AI engagements."
AIlice/cluster_11.py,Model Abstraction,"This code implements a multi-provider AI integration pattern, allowing dynamic configuration of various AI models, temperature, and context window ratios for individual users. It utilizes a role-based model assignment pattern for agents and manages AI interactions through a session-based approach, including persistent conversation history."
Copilot-Agent-365/cluster_4.py,MLOps,"This code implements an **AI memory management pattern** that leverages Azure File Storage to persist both a shared global memory and distinct, user-specific contextual memories. This enables an AI system to maintain state across interactions, providing personalized experiences by isolating individual user data while also accessing common knowledge. The robust handling of JSON and binary data ensures reliable storage and retrieval of diverse AI-related information."
AIlice/cluster_0.py,MLOps,"This code demonstrates patterns for hardware-accelerated AI model deployment, specifically configuring `llama-cpp-python`. It automatically detects and sets build arguments to leverage NVIDIA (CUDA), AMD (ROCm/HIPBLAS), or Vulkan-compatible hardware. This enables optimized local inference of large language models by tailoring the underlying `llama.cpp` library to the available compute resources."
AgentBench/cluster_13.py,MLOps,"This code implements a recursive serialization pattern crucial for efficient representation and summarization of complex data structures, a common requirement in AI for handling model configurations, intermediate states, or large output logs. It incorporates depth limiting and compression techniques to manage data verbosity, alongside robust handling of diverse object types, including custom AI-specific classes, for effective debugging and logging within AI systems."
AgentBench/cluster_3.py,MLOps,"This code implements a robust configuration management pattern, essential for defining and managing complex AI/ML experiments. It supports hierarchical and compositional configuration through `import` directives and allows for flexible parameter tuning via `default` and `overwrite` mechanisms. This declarative approach facilitates reproducible and modular specification of AI model architectures, hyperparameters, and training setups."
ChatSim/cluster_142.py,MLOps,"This code exemplifies a **Pluggable Algorithm** pattern, enabling dynamic selection and configuration of various AI tracking methods at runtime through command-line arguments. It facilitates the **Model Deployment and Inference** phase, where the chosen tracker processes video data to perform its designated object tracking task."
DLTA-AI/cluster_109.py,MLOps,"This code implements a robust **model export and optimization pipeline**, converting PyTorch models into various inference-optimized formats like TorchScript, ONNX, OpenVINO, TensorRT, and TFLite. It further features a **multi-backend inference pattern** through the `ReIDDetectMultiBackend` class, which dynamically loads and executes these diverse model formats, adapting runtime behavior and data preprocessing for each specific inference engine. This enables flexible deployment across different hardware and software environments."
DLTA-AI/cluster_16.py,MLOps,"This code implements a robust AI inference pattern, enabling the loading and execution of pre-trained detector models from configuration files and checkpoints. It supports both single-model and batch inference, incorporating a specific pattern for applying test-time data augmentation, such as image flipping, within the inference pipeline. The system then performs inference on input images and can optionally visualize the results."
EvoAgentX/cluster_23.py,MLOps,"This code implements a robust pattern for **AI artifact management**, specifically handling the persistent storage of diverse AI entity metadata such as agent configurations, workflow definitions, memory, and interaction history. It leverages **polymorphic data handling** by dynamically validating and processing these distinct AI artifact types using their respective Pydantic models, and employs **dynamic schema generation** to adapt database tables based on these evolving AI data structures."
chipper/cluster_1.py,MLOps,"This code manages environment configurations for AI applications, specifically those interacting with Large Language Models (LLMs). It includes patterns for configuring `OLLAMA_URL` for local LLM inference and `API_KEY` for service access. Furthermore, the presence of `.ragignore` and `.systemprompt` files indicates support for Retrieval Augmented Generation (RAG) and structured prompt engineering within the AI workflow."
chipper/cluster_5.py,MLOps,"This code configures an AI application, specifically setting up environment files for Large Language Model (LLM) interaction via `.systemprompt` and local inference using Ollama, indicated by `OLLAMA_URL` and GPU detection. It also prepares for a Retrieval-Augmented Generation (RAG) pattern by managing `.ragignore` files, which define data sources for retrieval, and secures access with dynamically generated API keys."
evadb/cluster_30.py,MLOps,"This code implements robust MLOps-aligned release automation patterns for an AI project. It orchestrates version bumping, dynamic changelog generation from Git history, and the building and publishing of Python packages (wheels, sdist) to PyPI and GitHub releases. This ensures a streamlined, traceable, and continuous delivery pipeline for the AI software's artifacts."
mcp-agent/cluster_6.py,MLOps,"The code demonstrates robust patterns for configuring AI observability components, specifically OpenTelemetry exporters. It supports flexible configuration schemas, including string, type-discriminated, and key-discriminated formats, allowing for polymorphic definition and multiple instances of exporters. Furthermore, it implements a layered configuration strategy, merging and deduplicating lists of AI-related modules and exporters from various sources to enable extensible and secure deployments."
mcp-agent/cluster_84.py,MLOps,"This code implements an **LLM workflow observability pattern** by parsing log records to identify and summarize distinct stages of AI processing, including LLM call lifecycle events, conversation events, and quality evaluations. It employs **event-driven logging** to extract specific AI-related metrics, such as token counts from LLM responses and scores from quality evaluations. This structured approach enhances the monitoring and understanding of complex AI system interactions."
nuplan-devkit/cluster_167.py,MLOps,"The code implements a robust framework for executing AI tasks, featuring `WorkerPool` implementations for sequential, single-machine parallel (threads/processes), and distributed (Ray) computation. It incorporates patterns for configuration-driven instantiation of AI models (e.g., planners, observations) from checkpoints, alongside dynamic scaling of optimizer and learning rate scheduler parameters to optimize distributed training performance across multiple GPUs and nodes. This architecture facilitates adaptable and scalable AI model development and evaluation."
nuplan-devkit/cluster_8.py,MLOps,"The code primarily showcases AI configuration management patterns using Hydra, orchestrating complex simulation and visualization pipelines typical in autonomous driving development. It includes robust data management for specialized AI datasets via `NuPlanDBWrapper`, ensuring efficient access and memory handling. Additionally, it demonstrates dynamic adjustment of AI training configurations, such as GPU allocation and data caching, to optimize experiment execution."
optillm/cluster_59.py,MLOps,"This code demonstrates a robust pattern for managing secure communication within an AI application. It centralizes SSL configuration, parsing verification status and custom certificate paths from CLI arguments and environment variables. This unified configuration is then consistently applied to various AI service clients (e.g., OpenAI, Cerebras, AzureOpenAI) by injecting a pre-configured `httpx.Client`, and also to internal plugins for secure external data integration."
optillm/cluster_60.py,MLOps,"The code validates configuration patterns critical for optimizing AI model serving, specifically focusing on batch processing. It tests the parsing and application of command-line arguments such as `--batch-mode`, `--batch-size`, and `--batch-wait-ms`. These parameters represent a common AI pattern for grouping multiple inference requests to enhance throughput and efficiency, particularly for large language models."
pybroker/cluster_13.py,MLOps,"This code implements a modular caching strategy, a common AI pattern, to optimize performance by storing frequently accessed components like raw data (`data_source_cache`), derived features (`indicator_cache`), and trained machine learning models (`model_cache`). This pattern leverages a global `StaticScope` to manage distinct cache instances, providing explicit control for enabling, disabling, and clearing each cache type individually or collectively, thereby reducing redundant computations and resource loading in AI workflows."
AgentBench/cluster_5.py,LLM-based User Intent Extraction,"This code exemplifies core AI patterns in **symbolic knowledge representation and semantic parsing**, utilizing Lisp-like logical forms as its primary data structure. It implements functionalities for **parsing, serializing, and structurally analyzing these symbolic expressions**, alongside **graph-based isomorphism checks** for comparing semantic equivalence. Furthermore, it incorporates **rule-based transformations** for refining and normalizing generated logical forms, crucial for tasks like knowledge graph querying and program synthesis."
AgentBench/cluster_8.py,LLM-based User Intent Extraction,"This code implements a recursive semantic parsing pattern, translating structured logical expressions into a `networkx` graph. This graph serves as a knowledge graph-based query representation, where nodes denote entities, literals, or classes, and edges represent relations. It supports complex query operations like joins, comparisons, aggregations (COUNT), and argument selection (ARGMAX/ARGMIN), crucial for knowledge base querying and reasoning."
EvoAgentX/cluster_74.py,LLM-based User Intent Extraction,"The code demonstrates an **AI tool use pattern**, where a central `MCPToolkit` orchestrates access to external, specialized AI-enabled services. It showcases the dynamic discovery and invocation of a specific ""arXiv search"" tool, allowing an agent to programmatically interact with an external knowledge source to retrieve information on a given query like ""artificial intelligence."" This pattern facilitates modular AI architectures by integrating diverse functionalities as callable tools."
clai/cluster_36.py,LLM-based User Intent Extraction,"The `Linuss` class implements a **Rule-based Intelligent Agent** pattern, functioning as a command transformation and suggestion engine. It acts as a specialized **Expert System** by applying predefined equivalency rules to user commands, while the `Datastore` and `Provider` classes establish a **Multi-source Information Retrieval System** for querying and integrating information from various external knowledge bases."
semantic-router/cluster_10.py,LLM-based User Intent Extraction,"This code implements a robust **semantic routing** framework, utilizing **embedding generation** to transform inputs into vectors for **vector similarity search**. It defines an abstract **vector index** for efficient storage and retrieval of these embeddings, supporting both synchronous and asynchronous operations. The system further incorporates **LLM-driven dynamic routing**, **data synchronization** between local route definitions and the remote index, and includes functionalities for **model training and evaluation** to optimize routing accuracy."
semantic-router/cluster_27.py,LLM-based User Intent Extraction,"This code implements semantic routing patterns, classifying user input into predefined routes like 'politics' or 'chitchat' by leveraging text embedding models such as Cohere's `embed-english-v3.0`. It supports various score aggregation methods (sum, mean, max) for robust route determination and allows for the integration of specific Large Language Models (LLMs) within individual routes, enabling a hybrid AI system."
semantic-router/cluster_28.py,LLM-based User Intent Extraction,"This code implements a **Semantic Router** pattern, classifying inputs into predefined `routes` by leveraging **embedding models** (e.g., Cohere's `embed-english-v3.0`) to compute semantic similarity with example `utterances`. It incorporates configurable **aggregation strategies** (sum, mean, max) for route scoring and supports the **integration of specific Large Language Models (LLMs)** within individual routes for specialized processing."
vigil-llm/cluster_9.py,LLM-based User Intent Extraction,"The `Vigil` class implements an **AI orchestration pattern**, dynamically configuring and managing core AI components such as a `VectorDB` and an `Embedder` based on external configurations. It utilizes a **plugin-based scanning architecture** where `input_scanners` and `output_scanners` are registered and instantiated via a `ScannerRegistry`. This design allows for modular AI-driven content analysis, with individual scanners optionally leveraging the central `VectorDB` and `Embedder` through a form of **dependency injection** for their operations."
optillm/cluster_16.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This codebase implements a collection of advanced AI patterns for optimizing LLM interactions, such as Best-of-N sampling with self-rating, Chain of Thought with explicit reflection, and Round-Trip Optimization for iterative code generation and verification. It also incorporates strategies like Inference-Time PV Games for generating and evaluating diverse solutions, Advanced Self-Consistency for robust answer selection, and a dynamic router that intelligently dispatches queries to the most appropriate reasoning approach (e.g., MCTS, RE2, PlanSearch)."
AgentBench/cluster_18.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code implements a preprocessing pipeline for Lisp-like logical forms, a common pattern in semantic parsing for natural language understanding. It recursively transforms hierarchical expressions by handling inverse relations and flattening complex relation paths, specifically for superlative operations like ARGMAX/ARGMIN. The core AI pattern involves linearizing these nested logical forms into a sequence of sub-formulas with associated hierarchical levels, preparing them for efficient bottom-up processing by neural models."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_6.py,"LLM-based Planning, XoT, ReAct, or Reasoning","The `CarlaEnvironment` implements a Reinforcement Learning environment with standard `reset()` and `step()` methods, providing a multi-modal observation space from camera and navigation sensors. It features a custom reward function designed to train an autonomous driving agent, supporting both continuous and discrete action spaces for vehicle control within a dynamic simulation populated by NPC agents."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_8.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code defines a Reinforcement Learning (RL) environment within the CARLA simulator, featuring a multi-modal observation space that integrates visual data from a front-facing camera with numerical navigation metrics. It employs a shaped reward function to guide an agent, penalizing collisions, lane deviations, and speed inconsistencies, while supporting both discrete and continuous action spaces for vehicle control. The environment further incorporates waypoint-based navigation for path following and dynamically spawns other agents like pedestrians and vehicles, creating a complex simulation for autonomous driving training."
CAAFE/cluster_1.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code demonstrates an AI pattern of iterative feature generation, leveraging a specified method (e.g., TabPFN) to derive features from datasets. It incorporates essential data preparation steps, including train-test splitting and handling categorical features for machine learning readiness. Furthermore, the `generate_features` function indicates interaction with a generative AI model to produce code and prompts based on the input data."
evadb/cluster_14.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code demonstrates advanced AI-aware query optimization patterns, including predicate pushdown and reordering, to efficiently execute queries involving AI functions like object detectors. It leverages a cost-based approach for predicate reordering, prioritizing less expensive AI operations, and extensively employs caching mechanisms to reuse results from AI function evaluations, such as HuggingFace models and LLMs, thereby significantly reducing redundant computations. These patterns aim to optimize the performance of analytical queries on unstructured data by minimizing AI model inference costs."
factorio-learning-environment/cluster_65.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code exemplifies a hierarchical planning pattern, where the `ConnectEntities` tool orchestrates complex connection tasks by decomposing them into specialized sub-tasks like pathfinding, entity resolution, and inventory management. It employs a strategy pattern to dynamically select connection logic (e.g., Fluid, Transport, Power) and incorporates specialized pathfinding heuristics, such as `try_straight_line_pathing`, for adaptive problem-solving in specific scenarios."
factorio-learning-environment/cluster_86.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code demonstrates **hierarchical planning** by decomposing complex factory automation into modular sub-goals, such as setting up power, fluid inputs, and solid item transport lines. It utilizes **knowledge-based planning** to adaptively construct production chains based on recipe requirements, dynamically placing and connecting entities. The system performs **state monitoring** and **constraint satisfaction** to verify successful resource flow and production outcomes."
fast-graphrag/cluster_9.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code implements a common AI pattern for **sparse data ranking and top-k selection**. It efficiently extracts and sorts non-zero scores from a sparse row vector, returning the corresponding indices and values in descending order of score. This pattern is crucial for post-processing in applications like recommendation systems, information retrieval, or multi-label classification, where identifying and prioritizing the most relevant or highest-confidence items from a vast, sparse output is essential."
optillm/cluster_47.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code implements a pattern for quantifying AI model's internal reasoning steps, specifically by extracting and counting tokens within `<think>` tags. This enables explicit chain-of-thought tracking and token cost management for the model's intermediate thought processes. The pattern is robust, handling scenarios like truncated reasoning blocks where the closing tag is absent, crucial for real-world AI inference outputs."
optillm/cluster_65.py,"LLM-based Planning, XoT, ReAct, or Reasoning","This code implements and tests a pattern for quantifying ""reasoning tokens"" within AI model outputs, specifically content enclosed by `<think>` tags. This directly supports Chain-of-Thought (CoT) prompting, where an LLM's internal thought process is externalized for inspection and analysis. By counting these tokens, the system enables precise cost estimation and performance optimization for the explicit reasoning steps."
transfuser/cluster_22.py,"LLM-based Planning, XoT, ReAct, or Reasoning","The code primarily implements a **Behavior Tree pattern** using `py_trees` to define complex, reactive, and hierarchical behaviors for simulated agents in autonomous driving scenarios. These trees orchestrate dynamic interactions, such as a cyclist crossing after an ego vehicle's turn, within a **scenario-based testing framework** that includes **criteria-based evaluation** (e.g., collision detection) to assess AI system performance. This approach is further supported by patterns for robust **actor spawning and placement** and **parameterized scenario templates** to cover diverse traffic situations."
DLTA-AI/cluster_37.py,LLM-based Multimodal Generative Prompting,"This code implements a **center-region-based label assignment strategy** for object detection, classifying proposals as positive, negative, or ignored based on their overlap with scaled ground truth bounding box regions. It employs a **priority-based assignment** mechanism to resolve ambiguities when multiple ground truths overlap, favoring smaller objects. The assignment process utilizes Intersection over Foreground (IoF) as a **metric-based filtering** mechanism and is designed as a modular component within a larger framework."
DLTA-AI/cluster_91.py,LLM-based Multimodal Generative Prompting,"This code implements `BitmapMasks` and `PolygonMasks` data structures, representing segmentation masks crucial for computer vision AI tasks. It provides a comprehensive suite of geometric transformations—such as rescaling, resizing, flipping, cropping, and padding—which are essential patterns for data augmentation and preprocessing in deep learning pipelines. Furthermore, it includes utilities for extracting bounding boxes, calculating mask areas, and converting between mask formats and to common AI tensor/array types for model input."
DLTA-AI/cluster_121.py,LLM-based Multimodal Generative Prompting,"This code implements a multi-object tracking system, likely an OCSort variant, utilizing Kalman filters for object state prediction. Its core AI pattern is a multi-stage data association process that constructs a comprehensive cost matrix by combining spatial overlap (IoU), motion consistency (velocity direction), and optionally appearance embeddings (Re-ID) and category matching, solved via linear assignment. This allows for robust matching of current detections to existing tracks across frames."
3DOD_thesis/cluster_0.py,LLM-based Multimodal Generative Prompting,"This code implements a data pipeline for 3D object detection, specifically following the Frustum PointNet pattern. It processes LiDAR point clouds by projecting them into 2D image bounding box frustums, then transforms and normalizes these frustums into a canonical coordinate space. This pipeline generates comprehensive multi-task labels, including instance segmentation masks, 3D bounding box parameters (center, dimensions, binned orientation with residual), and 3D corner coordinates, suitable for training a robust 3D detector."
3DOD_thesis/cluster_10.py,LLM-based Multimodal Generative Prompting,"This code defines data pipelines for 3D object detection, preparing inputs and ground truth for both frustum-based and image-based AI models. It showcases patterns like frustum generation, point cloud sampling and canonicalization for 3D networks, alongside image cropping and normalization for 2D CNNs. The pipelines generate diverse regression targets, including instance segmentation masks, 3D bounding box parameters (center, dimensions, orientation bins/residuals), and 2D projected 3D keypoints."
3DOD_thesis/cluster_8.py,LLM-based Multimodal Generative Prompting,"This code implements a robust data pipeline for 3D object detection, primarily utilizing frustum-based point cloud processing and 2D image-based 3D estimation. It integrates multi-modal data (LiDAR point clouds, 2D images, and 2D/3D labels) through complex geometric transformations, including frustum cropping, point cloud sampling, and coordinate system alignment. Extensive data augmentation, such as random shifts, flips, and rotations on point clouds and 2D bounding boxes, is applied to generate diverse training examples for robust 3D object property prediction."
3DOD_thesis/cluster_9.py,LLM-based Multimodal Generative Prompting,"This code implements a data pipeline for 3D object detection and tracking, primarily utilizing the KITTI dataset. It employs a frustum-based approach, projecting 3D LiDAR point clouds into 2D image bounding boxes to extract relevant 3D points. These frustum point clouds undergo critical preprocessing steps such as coordinate transformation, centering, and fixed-size sampling, alongside standard ImageNet normalization for corresponding 2D image crops, preparing multi-modal inputs for deep learning models."
AIlice/cluster_21.py,LLM-based Multimodal Generative Prompting,"This code implements a **multimedia data representation and preprocessing pattern** through the `AVideo` class, which encapsulates raw video bytes and automatically extracts essential metadata (format, dimensions, FPS) upon initialization. It further demonstrates a **data ingestion pattern** with `AVideoLocation`, enabling flexible loading of video content from local paths, URLs, or proxied sources. The `Standardize` methods, leveraging `ConvertVideoFormat`, exemplify a **data transformation pattern** crucial for preparing diverse video inputs into a consistent format (e.g., MP4) suitable for downstream AI model consumption."
AIlice/cluster_26.py,LLM-based Multimodal Generative Prompting,"This code implements an AI pattern for robotic process automation (RPA) by leveraging Optical Character Recognition (OCR) via `easyocr` to locate and interact with UI elements on a screen, enabling actions like clicking, scrolling, and typing based on recognized text. It further supports multimodal AI interactions by processing images and videos, extracting frames and encoding them for consumption by models like GPT-Vision, and manages conversational context, including the extraction of code snippets from user messages."
AIlice/cluster_27.py,LLM-based Multimodal Generative Prompting,"This code implements a modular AI service for speech processing, integrating specific Speech-to-Text (`S2T_WhisperLarge`) and Text-to-Speech (`T2S_ChatTTS`) models. It employs an asynchronous, multi-threaded producer-consumer pattern to manage the TTS pipeline, where text inputs are queued, processed by the TTS model in a dedicated thread, and the synthesized audio is subsequently queued for playback. This design decouples AI inference from I/O operations, enabling concurrent and responsive speech interactions."
AIlice/cluster_28.py,LLM-based Multimodal Generative Prompting,"This code implements a real-time speech processing system, integrating a **Speech-to-Text (S2T) pattern** via a Whisper-large model for robust audio transcription and a **Text-to-Speech (T2S) pattern** (ChatTTS) for speech synthesis. It utilizes an **asynchronous processing pipeline** with multi-threading and queues to manage continuous audio transcription, text-to-speech conversion, and playback, alongside explicit **device management** for optimized model inference."
AIlice/cluster_29.py,LLM-based Multimodal Generative Prompting,"This code implements core AI patterns for Text-to-Speech (TTS) and Speech-to-Text (STT), specifically utilizing `T2S_ChatTTS` for speech synthesis and `S2T_WhisperLarge` for speech recognition. These capabilities are integrated into an asynchronous, multi-threaded producer-consumer pipeline, where text is processed into audio via queues for non-blocking playback and real-time interaction."
AIlice/cluster_38.py,LLM-based Multimodal Generative Prompting,"This code demonstrates AI patterns for multimodal input formatting, specifically adapting text, image, and video attachments for different large language models like GPT-Vision and Claude-Vision. It employs a video processing pattern that extracts a fixed number of keyframes to represent video content, directly influencing the multimodal token estimation strategy. This highlights model-specific API adaptation for visual data and associated resource management."
AIlice/cluster_9.py,LLM-based Multimodal Generative Prompting,"This code implements a **data standardization pattern** for multimedia, where `AImageLocation` and `AVideoLocation` classes preprocess raw image and video data into consistent formats (e.g., RGB JPEG, MP4) suitable for AI model consumption. It also employs **AI-specific data types** (`AImage`, `AVideo`) that are centrally managed and served via a `/proxy` endpoint. This endpoint acts as a unified access point, intelligently handling both external resources and internal AI-ready data representations."
Automatic_Speech_Recognition/cluster_10.py,LLM-based Multimodal Generative Prompting,"This code implements a data preparation pipeline for audio-based AI tasks, specifically focusing on feature extraction and label encoding. It extracts numerical features from WAV audio files using `calcfeat_delta_delta` and normalizes them, a common preprocessing step for machine learning models. Concurrently, it performs label encoding by converting character-based transcriptions into numerical targets, including special tokens tailored for sequence-to-sequence AI architectures."
Automatic_Speech_Recognition/cluster_16.py,LLM-based Multimodal Generative Prompting,"This code implements the **feature extraction** pattern for audio processing by generating Mel filter banks. It converts frequencies between Hertz and the perceptually-motivated Mel scale, then constructs a set of triangular filters. This process is a foundational step in transforming raw audio signals into a more compact and relevant representation, commonly used for tasks like **speech recognition** and **audio classification**."
ChatSim/cluster_10.py,LLM-based Multimodal Generative Prompting,"This code demonstrates patterns for comprehensive data preprocessing in 3D perception, extracting and organizing multi-sensor data (images, LiDAR point clouds, camera/LiDAR calibrations, object labels, and map features) from TFRecord datasets for applications like autonomous driving or neural rendering. Additionally, it includes distinct patterns for evaluating Vision Transformer (ViT) models, profiling their computational efficiency (MACs, parameters) and inference speed (latency, FPS) on template and search image inputs, characteristic of visual tracking or recognition tasks."
ChatSim/cluster_139.py,LLM-based Multimodal Generative Prompting,"This code implements two distinct object localization patterns: a `Corner_Predictor` and a `CenterPredictor`. The `Corner_Predictor` uses separate convolutional branches to generate heatmaps for top-left and bottom-right corners, employing a differentiable soft-argmax to regress precise sub-pixel coordinates. Conversely, the `CenterPredictor` utilizes a multi-task head to predict an object's center heatmap, its size, and a local offset, deriving bounding boxes by selecting the highest center score and corresponding regressed values."
ChatSim/cluster_159.py,LLM-based Multimodal Generative Prompting,"This code implements a data augmentation pattern, specifically applying geometric transformations to images. It randomly performs horizontal flips, vertical flips, and 90-degree rotations on input images. This approach is commonly used in deep learning to expand training datasets and improve model generalization."
ChatSim/cluster_161.py,LLM-based Multimodal Generative Prompting,"This code implements a robust image resampling pattern, specifically utilizing bicubic interpolation for image resizing. It employs a separable approach, calculating interpolation weights and indices independently for height and width dimensions. Furthermore, it incorporates symmetric padding to effectively handle boundary conditions during the resampling process, preventing artifacts."
ChatSim/cluster_50.py,LLM-based Multimodal Generative Prompting,"This code implements a **stochastic agent movement pattern**, simulating an entity's position and velocity updates through probabilistic acceleration. It incorporates **boundary handling**, resetting the agent's velocity randomly upon hitting image dimensions. This pattern facilitates **random exploration** within a constrained space, driven by Gaussian or uniform random distributions for velocity and acceleration."
ChatSim/cluster_53.py,LLM-based Multimodal Generative Prompting,"This code implements an automatic instance segmentation pipeline utilizing a pre-trained Segment Anything Model (SAM) to generate masks from input images. It configures the mask generation process with specific AI parameters such as `points_per_side`, `pred_iou_thresh`, and `stability_score_thresh` to control the density and quality of the output. The generated masks, along with model-specific metadata like `predicted_iou` and `stability_score`, are saved in either binary image format or COCO RLE for efficient representation."
ChatSim/cluster_6.py,LLM-based Multimodal Generative Prompting,"This code implements AI patterns for processing and aligning 3D reconstruction data, specifically parsing camera intrinsics, extrinsics, and sparse point clouds from Structure-from-Motion (SfM) pipelines like COLMAP and Metashape. It performs geometric alignment by applying rigid body transformations to unify these diverse 3D datasets into a common coordinate system. This also includes integrating SfM-derived 3D points with LiDAR sensor data, demonstrating multi-modal 3D scene representation."
ChatSim/cluster_67.py,LLM-based Multimodal Generative Prompting,"This code implements a data generation pipeline for computer vision, specifically focusing on creating image-mask pairs. It utilizes configurable mask generation strategies (e.g., segmentation-based or mixed random masks) and applies AI data augmentation techniques such as resizing and random square cropping. The pipeline is designed for efficient, parallel processing of images to prepare datasets for training deep learning models, with configuration managed via Hydra for reproducibility."
DLTA-AI/cluster_0.py,LLM-based Multimodal Generative Prompting,"This code implements data preparation patterns crucial for computer vision tasks, specifically object detection and segmentation. It defines PASCAL VOC-like classes and provides functionality to convert image annotations, including bounding boxes and segmentation masks, into the widely adopted COCO dataset format. This conversion process standardizes datasets for training and evaluating machine learning models."
DLTA-AI/cluster_100.py,LLM-based Multimodal Generative Prompting,"This code pattern validates the structural integrity and configuration of various object detection bounding box heads, ensuring consistency in input/output channels, number of classes, and internal layer dimensions. It explicitly supports specialized heads like `SABLHead` and `DIIHead`, verifying their classification and regression branches, including class-agnostic options, against their defined configurations within a potentially modular, multi-stage architecture."
DLTA-AI/cluster_24.py,LLM-based Multimodal Generative Prompting,"This code implements a data splitting pattern essential for semi-supervised learning, specifically for frameworks like SoftTeacher. It categorizes a single batch of input data, including images and metadata, into distinct groups based on assigned 'tags' such as 'sup', 'unsup_teacher', and 'unsup_student'. This enables separate processing of supervised and different unsupervised data streams for training teacher and student models."
DLTA-AI/cluster_28.py,LLM-based Multimodal Generative Prompting,"This code establishes a modular deep learning framework for object detection and segmentation, implementing both single-stage (e.g., MaskFormer) and two-stage architectures that fuse instance and semantic predictions for panoptic segmentation. It further includes patterns for comprehensive performance evaluation and visualization of these AI model outputs, comparing ground truth with detected bounding boxes and masks."
DLTA-AI/cluster_38.py,LLM-based Multimodal Generative Prompting,"This code implements a specialized bounding box encoding and decoding pattern, crucial for point-based object detection models. It transforms standard `xyxy` bounding box coordinates into a set of four distances (left, top, right, bottom) relative to a given reference point. Conversely, it decodes these predicted distances back into `xyxy` bounding boxes, facilitating the training and inference of models that predict object extents from specific locations."
DLTA-AI/cluster_39.py,LLM-based Multimodal Generative Prompting,"This code implements a specialized bounding box encoding/decoding pattern, `DistancePointBBoxCoder`, which transforms standard `xyxy` bounding boxes into point-relative distances (top, bottom, left, right) and vice-versa, crucial for anchor-free object detection. The `TOODHead` leverages this by employing a multi-level feature pyramid for classification and regression, incorporating deformable convolutions for adaptive feature sampling. A key AI pattern is its ""Task Alignment Learning"" strategy, dynamically weighting classification and regression losses based on the alignment between predicted boxes and ground truth."
DLTA-AI/cluster_48.py,LLM-based Multimodal Generative Prompting,"The code showcases advanced AI patterns for robust object detection, including specialized loss functions like Gradient Harmonized Classification (GHMC) and Seesaw CrossEntropy, designed to mitigate gradient and class imbalance. It implements an anchor-free object detection head (`FSAFHead`) that dynamically assigns targets across multi-level features and employs a unique loss reweighting mechanism to select optimal feature pyramid levels for each ground truth object. These patterns are supported by modular component registration and flexible loss reduction utilities."
DLTA-AI/cluster_6.py,LLM-based Multimodal Generative Prompting,"This code implements a fundamental preprocessing pattern for image segmentation, converting vector-based annotations (polygons, rectangles, circles) into rasterized pixel-level masks. It specifically generates both semantic class maps and unique instance identification maps, a core requirement for training and evaluating instance segmentation models. This transformation prepares raw annotation data into ground truth labels directly consumable by deep learning architectures."
DLTA-AI/cluster_70.py,LLM-based Multimodal Generative Prompting,"This code defines a comprehensive collection of AI patterns for object detection and instance segmentation heads. It encompasses diverse approaches including anchor-based, anchor-free, keypoint-based, and transformer-based architectures, each employing specialized techniques for generating priors, assigning labels, and refining predictions. Key AI patterns observed are adaptive label assignment (ATSS, PAA, AutoAssign, Hungarian matching), advanced regression strategies (bucketing, distribution focal loss, point-based transformations), and enhanced feature processing via deformable convolutions and attention mechanisms, all optimized with task-specific loss functions."
DLTA-AI/cluster_71.py,LLM-based Multimodal Generative Prompting,"The code demonstrates diverse object detection head architectures, encompassing both anchor-based (SABLRetinaHead) and anchor-free designs (FoveaHead, GFLHead, RepPointsHead), all processing multi-scale features. Key AI patterns include specialized bounding box representations like bucketing (SABL), deformable points (RepPoints), and continuous distributions (GFL), coupled with adaptive feature sampling and alignment using deformable convolutions (RepPoints, TOOD, Fovea). These heads leverage advanced loss functions such as Quality Focal Loss and Distribution Focal Loss (GFL), and Task Alignment Learning (TOOD) to improve classification and localization quality."
FuxiCTR/cluster_51.py,LLM-based Multimodal Generative Prompting,"This code implements a robust **feature engineering pipeline** for AI datasets, utilizing a `feature_encoder` to read, preprocess, fit, and transform data. It incorporates essential **data splitting strategies** (random or sequential) to create train, validation, and test sets. Furthermore, it employs **parallel data processing** for efficient transformation of large data blocks into optimized parquet files, demonstrating a scalable approach to dataset preparation."
MIRNetv2/cluster_12.py,LLM-based Multimodal Generative Prompting,"This code implements robust data loading and preprocessing patterns for diverse image and video AI tasks, leveraging PyTorch's `Dataset` API. It features extensive data augmentation techniques, including geometric transformations (flips, rotations, random cropping), temporal sampling for video sequences, and task-specific noise injection for denoising. The patterns facilitate efficient preparation of various input formats, such as paired low-quality/ground-truth images, dual-pixel data, and optical flow, into normalized PyTorch tensors for deep learning model training."
MIRNetv2/cluster_6.py,LLM-based Multimodal Generative Prompting,"This code implements an image resampling pattern, specifically bicubic interpolation with anti-aliasing, crucial for preparing image data in AI applications. It calculates precise interpolation weights and indices, handling boundary conditions through symmetric padding to ensure high-quality resized outputs. This pattern is vital for robust image preprocessing before feeding data into deep learning models, preventing artifacts and maintaining data integrity."
RealtimeVoiceChat/cluster_5.py,LLM-based Multimodal Generative Prompting,"This code implements a real-time speech-to-text processing pipeline, featuring voice activity detection to manage speech and silence segments. It incorporates advanced sentence boundary detection, utilizing punctuation, timing heuristics, and text similarity to identify and yield potential sentence completions from partial transcriptions. Optionally, it integrates turn-taking detection to dynamically adjust silence thresholds, optimizing for conversational flow."
dia/cluster_2.py,LLM-based Multimodal Generative Prompting,"This code represents a generative AI system, likely a text-to-audio model, integrating a core Transformer-based architecture with a specialized Descript Audio Codec (DAC) for multi-modal processing. It employs advanced generative techniques such as Classifier-Free Guidance (CFG) and stochastic sampling (temperature, top-p, top-k) within an auto-regressive decoding loop. A distinct architectural pattern involves a ""delay pattern"" for audio tokens, managed by dedicated functions, to handle temporal dependencies during generation and reconstruction."
discrete_distribution_networks/cluster_9.py,LLM-based Multimodal Generative Prompting,"This code implements a collection of AI-driven ""Sampler"" patterns, each designed to evaluate and select generated images based on specific criteria. These patterns leverage diverse AI techniques, including L2-based pixel similarity (e.g., masked, noisy, quantized, color-averaged, super-resolution), classical computer vision for edge detection (Canny, Sobel), and deep learning models for feature extraction (ResNet for style, CIFAR for classification, CLIP for text-image alignment). An ensemble pattern, `MultiGuidedSampler`, further combines weighted evaluations from multiple samplers to provide multi-modal guidance for image selection."
evadb/cluster_18.py,LLM-based Multimodal Generative Prompting,"The code demonstrates patterns for integrating AI/ML models directly into a database-like system, enabling in-database inference and feature extraction on various media types (video, image, audio, PDF) through custom AI functions. It implements optimized media data ingestion from local and cloud storage (S3), alongside advanced data sampling (e.g., I-frames, every Kth frame) and temporal/spatial segmentation strategies for efficient AI processing."
factorio-learning-environment/cluster_8.py,LLM-based Multimodal Generative Prompting,"This code implements a qualitative spatial reasoning pattern, converting continuous 2D coordinates into discrete, symbolic relative position descriptions within a bounding box. This serves as a fundamental AI feature engineering technique, enabling systems to interpret and reason about spatial relationships at a higher, more abstract level for tasks like object localization or scene understanding."
monkey-net/cluster_9.py,LLM-based Multimodal Generative Prompting,"This code implements a system for **keypoint-driven dense motion prediction**. It leverages **probabilistic keypoint representations**, transforming keypoints into Gaussian heatmaps and extracting mean/variance from predicted heatmaps. These keypoint representations are then used to generate rich **movement embeddings** (including difference heatmaps, keypoint displacement vectors, and warped source images) which feed into an Hourglass network to predict dense optical flow and image deformations."
nuplan-devkit/cluster_13.py,LLM-based Multimodal Generative Prompting,"This code defines data structures (`LidarBox`, `Box3D`) for representing tracked 3D objects, including their current state and derived motion properties like velocity and angular velocity from temporal data. A prominent AI pattern is multi-modal trajectory prediction, where functions like `get_future_box_sequence` and the `Box3D` class explicitly manage future positions, orientations, and associated probabilities (`mode_probs`) for these tracked objects. This enables the representation of predicted future states for agents within a scene."
nuplan-devkit/cluster_14.py,LLM-based Multimodal Generative Prompting,"The code implements a robust framework for object tracking and state estimation, calculating dynamic properties like velocity and angular velocity for `LidarBox` instances and managing their temporal relationships. It further incorporates advanced AI patterns for trajectory planning and prediction, leveraging kinematic motion models, the Intelligent Driver Model (IDM) for agent behavior, and optimal control (iLQR/LQR) for ego vehicle trajectory tracking. Additionally, the system includes extensive data preparation routines, transforming tracked objects and ego states into structured tensors with relative pose and dynamic features, suitable for machine learning applications."
nuplan-devkit/cluster_28.py,LLM-based Multimodal Generative Prompting,"The code demonstrates AI patterns for processing and representing autonomous driving sensor data, specifically handling Lidar point clouds and camera images. This includes filtering, feature extraction (e.g., intensity, ring, lidar index), and converting geometric properties into numerical arrays and quaternions for AI model consumption. Furthermore, it showcases patterns for object detection, tracking, and motion prediction through the management of bounding boxes, generation of future object sequences, and computation of ego vehicle and object trajectories."
nuplan-devkit/cluster_34.py,LLM-based Multimodal Generative Prompting,"The code reveals a pattern of multi-modal sensor data aggregation, where a `Log` object collects and provides unified access to diverse sensor inputs like images, LiDAR point clouds, and associated bounding boxes from various sub-components (cameras, lidars). This hierarchical data management is fundamental for AI systems in domains such as autonomous driving, enabling the fusion of complex sensor streams for perception and decision-making."
nuplan-devkit/cluster_61.py,LLM-based Multimodal Generative Prompting,"This code implements **vectorized HD map feature extraction and encoding patterns** for autonomous driving AI, transforming raw map data (e.g., lanes, intersections, stop lines) into structured representations like lane segments and polylines. It further incorporates **dynamic context encoding** for elements such as traffic light statuses and route information, localizing all features relative to the ego vehicle to provide comprehensive inputs for neural network-based perception and planning models."
tf.fashionAI/cluster_24.py,LLM-based Multimodal Generative Prompting,"This code implements common image preprocessing and data augmentation patterns crucial for deep learning models. It distinctly applies random cropping (leveraging `tf.image.sample_distorted_bounding_box`) and random horizontal flipping during training to enhance model generalization. For inference, it employs deterministic steps like aspect-ratio-preserving resizing and central cropping, followed by mean image subtraction for normalization, all orchestrated within a TensorFlow computational graph."
v2x-vit/cluster_12.py,LLM-based Multimodal Generative Prompting,"This codebase implements distinct multi-agent perception AI patterns: Early Fusion, Intermediate Fusion, and Late Fusion. Early Fusion aggregates raw LiDAR point clouds from multiple vehicles into a single input for a unified model. Intermediate Fusion processes individual vehicle data to extract features, which are then merged and aligned before the final detection head. Late Fusion involves independent processing by each vehicle, with their individual predictions combined at a later stage, all while incorporating simulations of real-world communication delays and localization errors for robust training."
v2x-vit/cluster_23.py,LLM-based Multimodal Generative Prompting,"The code implements various PointPillar-based 3D object detection models, processing LiDAR data through voxel feature encoding and scattering to a Bird's Eye View (BEV) representation. A prominent AI pattern is cooperative perception, utilizing diverse fusion networks such as V2VNetFusion, SpatialFusion, AttFusion, and V2XTransformer to aggregate features from multiple agents. These fusion strategies incorporate spatial alignment, prior encodings, and advanced mechanisms like attention and Transformer architectures for robust multi-vehicle perception, culminating in anchor-based classification and regression heads."
viral-clips-crew/cluster_1.py,LLM-based Multimodal Generative Prompting,"This code implements a multi-stage AI pipeline, starting with speech-to-text transcription using a Whisper-like model for both YouTube and local video content. It then feeds the transcribed data into an AI agent orchestration framework (e.g., CrewAI) for further processing, such as content extraction or summarization. This workflow demonstrates a pattern of chaining specialized AI models for comprehensive media processing and content creation."
build_MiniLLM_from_scratch/cluster_5.py,LLM-based Context Management,"This code showcases fundamental conversational AI patterns, managing chat history to maintain context for subsequent model interactions. It implements both synchronous and asynchronous text generation, leveraging tokenization and decoding for model input/output. A key pattern demonstrated is streaming output, providing real-time, token-by-token responses for an enhanced user experience."
AIlice/cluster_23.py,"LLM Fine-Tuning, Training & Alignment","The code implements a unified interface for interacting with diverse Large Language Models (LLMs), abstracting both API-based services (e.g., OpenAI, Anthropic, Mistral) and locally hosted Hugging Face Causal LMs. This pattern includes dynamic model loading with quantization and PEFT support for local models, alongside standardized conversation formatting and real-time streaming generation with token consumption tracking for all integrated LLMs."
AgentBench/cluster_19.py,"LLM Fine-Tuning, Training & Alignment","This code implements an agent-environment interaction loop for the ALFWorld task, characteristic of reinforcement learning, where an agent receives observations and takes actions. A core AI pattern is the robust action processing, which utilizes BLEU score for fuzzy matching to align the agent's generated actions with a set of admissible commands. This enables flexible interpretation of agent outputs, particularly when integrating with language models that generate natural language actions."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_2.py,"LLM Fine-Tuning, Training & Alignment","This code implements a Proximal Policy Optimization (PPO) agent, characterized by its actor-critic architecture. It leverages a clipped surrogate objective and an ""old policy"" to achieve stable policy updates, alongside an experience replay buffer for data collection. The learning process incorporates entropy regularization to encourage exploration and a value function loss for critic training."
DLTA-AI/cluster_18.py,"LLM Fine-Tuning, Training & Alignment","This codebase primarily implements advanced AI patterns for object detection, prominently featuring Knowledge Distillation where a student model learns from a pre-trained teacher. It demonstrates the modular integration and testing of various detection architectures (e.g., Faster R-CNN, DETR, MaskFormer) and specialized loss functions. Additionally, the code incorporates hard example mining techniques like OHEM and ScoreHLRSampler to optimize training by focusing on challenging samples."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_0.py,Forecasting with Classical Models,"This code implements a Proximal Policy Optimization (PPO) agent for reinforcement learning, interacting with a Carla simulation environment. It features patterns for state encoding, dynamic action standard deviation decay for exploration, and a structured training loop. Comprehensive experiment tracking via TensorBoard logs metrics such as episodic reward, deviation from center, and distance covered, alongside robust checkpointing and model loading capabilities."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_1.py,Forecasting with Classical Models,"This code implements a Deep Q-Network (DQN) or Double DQN (DDQN) agent for reinforcement learning, trained within a simulated Carla environment. It employs a standard RL training loop where the agent interacts with the environment, processes observations into a latent representation, stores experiences in a replay buffer, and updates its policy. The system also integrates robust experiment tracking with `SummaryWriter` and checkpointing for monitoring and resuming long-running training sessions."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_4.py,Forecasting with Classical Models,"This code implements a Deep Q-Network (DQN) agent, featuring a Dueling DQN architecture that separates value and advantage streams for robust Q-value estimation. It incorporates essential reinforcement learning patterns including experience replay with a `ReplayBuffer`, a periodically updated target network for stable learning, and epsilon-greedy exploration for action selection."
ChatSim/cluster_106.py,Forecasting with Classical Models,"This code exemplifies the **Batch Normalization** pattern, comparing the forward and backward passes of a PyTorch `nn.BatchNorm2d` layer against a meticulously hand-crafted normalization implementation. It leverages **automatic differentiation** to verify that both the normalized outputs and their corresponding gradients are identical, a critical pattern for validating custom AI layer correctness."
ChatSim/cluster_51.py,Forecasting with Classical Models,"This code exemplifies a synthetic data generation pattern, creating artificial video sequences of randomly shaped objects. It employs kinematic simulation to model the object's motion across frames, generating binary masks that serve as ground truth for computer vision tasks such as object tracking or segmentation."
ChatSim/cluster_76.py,Forecasting with Classical Models,"This code implements a **Perceptual Loss** pattern, leveraging a pre-trained VGG19 network to compute feature-level differences between generated and target images, often used in image synthesis tasks. It replaces `MaxPool2d` with `AvgPool2d` in the VGG backbone and calculates MSE loss on features extracted from ReLU layers. Complementing this, the code provides **AI model visualization utilities** to display masked images and generated outputs, crucial for evaluating and debugging image manipulation models."
MiniMax-M1/cluster_0.py,Forecasting with Classical Models,"This code implements a specialized multi-head attention mechanism that incorporates a decaying attention kernel and block-wise processing for sequence handling. It employs a recurrent state update pattern for efficient inference, where key-value states are exponentially decayed and accumulated, and the final output is modulated by a sigmoid gating mechanism."
Automatic_Speech_Recognition/cluster_1.py,Forecasting with Classical Models,"This code outlines a TensorFlow 1.x-based deep learning pipeline for sequence transcription, specifically designed for speech recognition tasks by processing character and phoneme-level data. It employs recurrent neural network (RNN) architectures, utilizing sparse tensor representations for efficient handling of variable-length target sequences and calculating performance metrics like Character/Phoneme Error Rate via edit distance. The system incorporates standard deep learning practices including data batching, shuffling, gradient-based optimization, and model checkpointing, alongside domain-specific phoneme mapping for normalization."
Automatic_Speech_Recognition/cluster_12.py,Forecasting with Classical Models,"This code implements a DeepSpeech2-inspired architecture, featuring a convolutional neural network (CNN) front-end for spectrogram feature extraction, followed by multiple stacked recurrent neural network (RNN, GRU, or LSTM) layers. It employs Connectionist Temporal Classification (CTC) for end-to-end sequence prediction, utilizing `ctc_loss` for training and `ctc_beam_search_decoder` for inference, alongside standard regularization (batch normalization, dropout) and optimization (Adam with gradient clipping) techniques."
Automatic_Speech_Recognition/cluster_14.py,Forecasting with Classical Models,"This code implements a foundational AI pattern for audio feature extraction, beginning with essential pre-processing steps like pre-emphasis and signal framing. It then transforms these framed audio segments into the frequency domain using Fast Fourier Transform to compute spectral power. A key AI pattern is the subsequent application of Mel-frequency filter banks to generate perceptually-weighted features, which are fundamental inputs for speech recognition and other audio-based machine learning models."
Automatic_Speech_Recognition/cluster_5.py,Forecasting with Classical Models,"This code implements a recurrent neural network (RNN) training and evaluation pipeline, designed for sequence-to-sequence or sequence labeling tasks at character ('cha') and phoneme ('phn') levels. It leverages configurable RNN architectures, mini-batch optimization with gradient clipping, model checkpointing, and edit distance-based error rates for performance assessment."
Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning/cluster_7.py,Forecasting with Classical Models,"This code implements a Variational Autoencoder (VAE) pattern for unsupervised learning, specifically designed for image data. It employs a characteristic VAE loss function combining reconstruction error with a Kullback-Leibler divergence term to regularize the latent space. The training pipeline follows standard deep learning practices, including data augmentation, batch processing, and distinct training, validation, and testing phases."
ChatSim/cluster_114.py,Forecasting with Classical Models,"This code implements a **feature extraction pattern** using a **pre-trained InceptionV3 convolutional neural network**, allowing selection of intermediate layer outputs for **transfer learning** applications. It specifically includes a **customized InceptionV3 variant for Fréchet Inception Distance (FID) computation**, demonstrating the adaptation of standard models for specific AI metric evaluation by patching internal modules and loading specialized weights."
ChatSim/cluster_146.py,Forecasting with Classical Models,"This code implements a scalable Approximate Nearest Neighbor (ANN) search system for high-dimensional embeddings, a fundamental AI pattern for representing complex data. It dynamically applies different ANN strategies based on dataset size, ranging from brute-force for small pools to asymmetric hashing and hierarchical partitioning (tree-based indexing) for larger datasets. These patterns, including vector normalization and reordering, optimize efficient similarity retrieval for AI applications."
ChatSim/cluster_4.py,Forecasting with Classical Models,"The code implements a U-Net architecture (`Model`) for Denoising Diffusion Probabilistic Models (DDPMs), characterized by its use of residual blocks, attention mechanisms, and sinusoidal timestep embeddings to process time-dependent inputs. Complementing this, it defines distinct `Encoder` and `Decoder` modules, indicative of a Variational Autoencoder (VAE) for learning latent representations, which also leverage residual connections and attention. This combination suggests a multi-stage generative pattern, likely a Latent Diffusion Model, where the VAE compresses data into a latent space for the U-Net-based diffusion process."
ChatSim/cluster_79.py,Forecasting with Classical Models,"This code implements a Generative Adversarial Network (GAN) architecture for image inpainting within a PyTorch Lightning module. It utilizes separate generator and discriminator networks, optimized through a multi-objective loss function combining adversarial, L1, perceptual, and feature matching components. Key training patterns include Exponential Moving Average (EMA) for the generator, dynamic input resizing, and a ""fake fakes"" mechanism to enhance model stability and output quality."
ChatSim/cluster_84.py,Forecasting with Classical Models,"The code implements a family of highly configurable deep learning generators, predominantly utilizing the **Residual Network (ResNet)** pattern as a core building block. It integrates advanced convolutional techniques such as **dilated convolutions** (including multi-dilated and depthwise separable variants) for expanded receptive fields, and **Frequency-aware Fourier Feature Convolutions (FFC)** to process information across spatial and frequency domains. These patterns are combined within modular encoder-decoder architectures, supporting adaptive channel configurations and flexible block specifications for diverse generative tasks."
ChatSim/cluster_96.py,Forecasting with Classical Models,"This code defines a MobileNetV2 convolutional neural network, leveraging Inverted Residual Blocks and depthwise separable convolutions for efficient feature extraction. It incorporates common AI patterns such as Batch Normalization and ReLU6 activation for stable training, alongside a specific weight initialization scheme and global average pooling before the final classification layer. The architecture also supports model scalability through a `width_mult` parameter."
ChatSim/cluster_97.py,Forecasting with Classical Models,"The code primarily showcases two distinct AI patterns: a Pyramid Pooling Module (PPM) and deep supervision. The `PPMDeepsup` class implements the PPM pattern to aggregate multi-scale contextual information from feature maps using adaptive average pooling and subsequent convolutions. Both `PPMDeepsup` and `C1DeepSup` utilize a deep supervision pattern, generating auxiliary outputs from an intermediate convolutional layer (`conv_out[-2]`) alongside the main output from the deepest layer (`conv_out[-1]`) to enhance training stability."
DLTA-AI/cluster_110.py,Forecasting with Classical Models,"This code implements a core component for multi-object tracking association, primarily leveraging a linear assignment problem (LAP) solver. The `min_cost_matching` function computes an optimal bipartite matching between tracks and detections, utilizing a configurable distance metric and a gating threshold to filter implausible associations. This pattern efficiently assigns detections to existing tracks while categorizing unmatched entities."
DLTA-AI/cluster_116.py,Forecasting with Classical Models,"This code primarily implements the Residual Network (ResNet) architecture, showcasing both BasicBlock and Bottleneck residual units. These blocks are characterized by sequential convolutional layers (3x3 and 1x1), batch normalization, and ReLU activations, critically incorporating skip connections that add the input directly to the block's processed output. This pattern facilitates the construction of deep networks by mitigating vanishing gradients and improving information flow."
DLTA-AI/cluster_31.py,Forecasting with Classical Models,"This code implements patterns for distributed deep learning, primarily focusing on synchronizing model states across multiple processes. It provides a generic `all_reduce_dict` function for aggregating dictionary values, which is specifically utilized by `SyncNormHook` to periodically average normalization layer statistics (e.g., BatchNorm's running mean/variance) during distributed training. This pattern ensures consistent global statistics for normalization layers, crucial for stable and effective training in multi-GPU environments."
DLTA-AI/cluster_33.py,Forecasting with Classical Models,"This code demonstrates advanced AI patterns for object detection and segmentation, emphasizing sophisticated RoI-based refinement strategies like Cascade R-CNN, Hybrid Task Cascade, and Sparse R-CNN, which iteratively improve predictions. It integrates specialized anchor/proposal generation and assignment techniques such as Guided Anchoring, Probabilistic Anchor Assignment (PAA), and hard example mining (OHEM, Score-HLR) for robust model training. Additionally, the code includes patterns for adaptive training (Dynamic R-CNN), fine-grained mask refinement (PointRend), and efficient inference with test-time augmentation."
DLTA-AI/cluster_62.py,Forecasting with Classical Models,"This code implements a Deformable DETR transformer decoder that iteratively refines object queries and bounding box predictions across its layers. It leverages deformable attention by dynamically adjusting sampling points based on reference points and valid ratios, enhancing efficiency for object detection. The integrated head applies multiple classification and regression branches, optionally supporting a two-stage detection pattern for improved proposal generation."
DRL-Pytorch/cluster_5.py,Forecasting with Classical Models,"This code implements and evaluates three distinct Reinforcement Learning (RL) algorithms: TD3 (Twin Delayed Deep Deterministic Policy Gradient), PPO (Proximal Policy Optimization), and SAC (Soft Actor-Critic). It demonstrates core RL patterns including agent-environment interaction within continuous control Gym environments, reward shaping via `Reward_adapter`, and action space adaptation. The framework also incorporates standard practices like seeding for reproducibility, TensorBoard logging, and periodic model saving and evaluation."
DRL-Pytorch/cluster_6.py,Forecasting with Classical Models,"This code implements a Deep Reinforcement Learning framework, supporting variants like Deep Q-Networks (DQN), Double DQN (DDQN), and Categorical DQN (C51). It utilizes an experience replay buffer for training stability and an epsilon-greedy exploration strategy with decaying noise to balance exploration and exploitation. The `render_policy` function specifically visualizes the predicted value distributions, a key characteristic of the C51 distributional RL algorithm."
FuxiCTR/cluster_54.py,Forecasting with Classical Models,"The code defines a deep learning framework centered around a `BaseModel` that encapsulates standard training patterns, including explicit training/evaluation loops, gradient clipping, and configurable optimizers and loss functions. It integrates crucial optimization and regularization techniques such as L1/L2 regularization for both embedding and network parameters, early stopping based on monitored metrics, and learning rate reduction on plateau. Extending this, the `MultiTaskModel` specifically implements a multi-task learning pattern, enabling a single model to address multiple prediction tasks with task-specific outputs, individual loss functions, and aggregated evaluation."
FuxiCTR/cluster_8.py,Forecasting with Classical Models,"This code implements a sophisticated pattern for feature representation learning, primarily through the integration of pre-trained embeddings. It supports various fusion strategies (initialization, summation, concatenation) with ID embeddings, alongside options to freeze pre-trained weights. A modular `FeatureEmbeddingDict` pattern dynamically manages embeddings for diverse feature types (categorical, sequence, numeric), incorporating feature-specific encoders and embedding sharing for efficient and flexible model construction."
GoBigger/cluster_16.py,Forecasting with Classical Models,"This code implements a **preprocessing and postprocessing hook pattern** specifically tailored for Long Short-Term Memory (LSTM) networks. The `_before_forward` method manages the crucial **initialization and batching of LSTM hidden and cell states**, including zero-initialization for the first sequence element. Conversely, `_after_forward` handles the **formatting and aggregation of the LSTM's output states**, providing flexibility in how the next hidden and cell states are represented."
GoBigger/cluster_17.py,Forecasting with Classical Models,"This code implements a Multi-Head Attention mechanism, a core pattern in Transformer architectures. It projects input embeddings into multiple query, key, and value representations, then computes independent scaled dot-product attention for each head. The outputs from these heads are subsequently concatenated and linearly transformed, with masking applied to manage sequence dependencies."
HuixiangDou/cluster_38.py,Forecasting with Classical Models,"This code implements a pattern for managing user-specific AI contexts, where each user is associated with a unique `feature_store_id` and a `qa_name` (knowledge base name). These identifiers are crucial for provisioning and accessing dedicated AI resources, such as a personalized feature store or a specific question-answering library. Authentication leverages JWTs to securely transmit these AI-related identifiers, ensuring subsequent AI operations are scoped to the user's specific context."
MIRNetv2/cluster_15.py,Forecasting with Classical Models,"This code implements the Natural Image Quality Evaluator (NIQE), a blind image quality assessment (IQA) method that extracts Asymmetric Generalized Gaussian Distribution (AGGD) features from image blocks and models them using Multivariate Gaussian distributions. It further includes standard full-reference metrics like PSNR and SSIM, with SSIM employing local statistical comparisons and 3D convolutional operations for comprehensive image quality evaluation."
OpenLearning4DeepRecsys/cluster_0.py,Forecasting with Classical Models,"This code implements a neural collaborative filtering (NCF) model, specifically NeuMF, for recommendation tasks, leveraging TensorFlow for its deep learning architecture. It employs common AI training patterns such as mini-batch gradient descent and likely negative sampling for implicit feedback. The model's performance is rigorously evaluated using recommender-specific metrics like Hit Ratio and NDCG, with periodic monitoring to identify the best performing iteration."
Reinforcement_Learning_in_Python/cluster_2.py,Forecasting with Classical Models,"This code implements the SARSA (State-Action-Reward-State-Action) reinforcement learning algorithm, a model-free, on-policy temporal-difference control method. It maintains a Q-table to store and update state-action values, employing an epsilon-greedy policy for action selection to balance exploration and exploitation. The learning rule specifically uses the *next chosen action* (`next_action`) to update the current state-action value, characteristic of SARSA's on-policy approach."
Reinforcement_Learning_in_Python/cluster_3.py,Forecasting with Classical Models,"This code implements the SARSA (State-Action-Reward-State-Action) reinforcement learning algorithm. It employs an epsilon-greedy policy for action selection, balancing exploration of the environment with exploitation of learned optimal actions. The core learning update rule is on-policy, directly using the Q-value of the *next action taken* to update the current state-action value."
VectorizedMultiAgentSimulator/cluster_28.py,Forecasting with Classical Models,"This codebase defines a vectorized reinforcement learning environment, featuring a `World` that simulates physics for `Entity` and `Agent` objects using batched PyTorch tensor operations. It incorporates explicit AI patterns such as PID controllers (`VelocityController`) for achieving target velocities and specialized agent dynamics models (`Drone`) that translate high-level actions into physical forces and torques. This architecture facilitates efficient training and evaluation of agents in physics-driven virtual environments."
VectorizedMultiAgentSimulator/cluster_6.py,Forecasting with Classical Models,"This codebase implements diverse multi-agent reinforcement learning scenarios centered around the cooperative control of physically coupled systems. Each scenario features two agents connected by a joint, tasked with manipulating an object or navigating through obstacles. Reward functions consistently employ dense shaping for positional, rotational, and speed objectives, often incorporating penalties for collisions and energy expenditure."
WebShop/cluster_22.py,Forecasting with Classical Models,"This code implements a Reinforcement Learning (RL) agent that integrates Natural Language Processing (NLP) capabilities, leveraging a BERT-based architecture for processing textual observations and goals. It employs an actor-critic or policy gradient RL framework, evidenced by the calculation of discounted rewards and advantages, and distinct policy gradient (`loss_pg`) and temporal difference (`loss_td`) loss components. The agent utilizes various action selection strategies (softmax, greedy, epsilon-greedy) and builds a comprehensive state representation from diverse inputs, including text and potential image features, to interact with its environment."
clai/cluster_34.py,Forecasting with Classical Models,"The code implements an agent-based AI system where `Agent` instances, such as `DemoAgent`, process user `State` and generate `Action` suggestions using explicit rule-based logic. It includes a robust data collection mechanism (`ActionRemoteStorage`, `StatsTracker`, `TerminalReplayMemory`) designed to capture user interactions and agent responses, which is foundational for training or evaluating reinforcement learning models. An `AgentDatasource` and `MessageHandler` manage the dynamic loading, selection, and orchestration of these AI agents."
deep_recommenders/cluster_10.py,Forecasting with Classical Models,"This code implements a streaming top-k retrieval pattern, designed to efficiently find the highest-scoring items from large datasets. It processes candidates in batches using `tf.data.Dataset`, applying an optional query embedding model and matrix multiplication for scoring. A reduction pattern then continuously merges and re-selects batch-wise top-k results to maintain the overall top-k across the entire dataset."
deep_recommenders/cluster_12.py,Forecasting with Classical Models,"This code implements several advanced recommendation system patterns, including Factorization Machines (FM), Wide & Deep Learning (WDL), and DeepFM. It demonstrates combining linear models with second-order feature interactions and deep neural networks for prediction. Notably, the FNN pattern initializes its linear and embedding layers by warming up with pre-trained weights from an FM model, while DeepFM integrates FM and DNN by sharing feature embeddings as input to the deep component."
drl_grasping/cluster_12.py,Forecasting with Classical Models,"This code defines two distinct Reinforcement Learning (RL) environments, `Grasp` and `Reach`, both implementing a standard RL interface with explicit action and observation space definitions. The `Grasp` environment incorporates advanced RL patterns like observation stacking for temporal context, curriculum learning for progressive task difficulty, and expert demonstrations for preloading replay buffers. The `Reach` environment focuses on reward shaping techniques, including sparse rewards and penalties for slow actions, to guide the agent towards a target."
drl_grasping/cluster_13.py,Forecasting with Classical Models,"The code defines a reinforcement learning environment (`Reach`) with a standard Gym-like interface, utilizing both sparse and dense reward shaping, including penalties, to guide agent learning. It prominently features an adaptive curriculum learning strategy (`GraspCurriculum`) that dynamically adjusts task difficulty by modifying environment parameters like required reach distance and object count based on the agent's success rate. This curriculum further decomposes the complex task into sequential sub-goals (REACH, TOUCH, GRASP, LIFT) with staged rewards."
drl_grasping/cluster_18.py,Forecasting with Classical Models,"This code defines a robotic manipulation task, structured with abstract methods for defining reinforcement learning action and observation spaces, rewards, and termination conditions. It leverages advanced robotic control patterns, including MoveIt2 for motion planning and servoing, coupled with robust spatial reasoning for coordinate frame transformations and environment perception from a simulated world. This enables AI agents to interact with the environment, manage robot state, enforce workspace constraints, and detect collisions for goal-oriented manipulation."
ladi-vton/cluster_4.py,Forecasting with Classical Models,"This code implements a virtual try-on (VTO) system built upon a Stable Diffusion architecture, leveraging components like `UNet2DConditionModel`, `AutoencoderKL`, and `DDPMScheduler` for image generation and inpainting. A key AI pattern is the use of an `InversionAdapter` that transforms CLIP vision features of clothing into text encoder word embeddings, enabling multimodal conditioning of the diffusion model. Additionally, the system incorporates an optional `EMASC` module for feature enhancement and utilizes `accelerator` for efficient distributed training and inference."
monkey-net/cluster_0.py,Forecasting with Classical Models,"The code implements a Generative Adversarial Network (GAN) architecture, featuring distinct `GeneratorFullModel` and `DiscriminatorFullModel` components engaged in an adversarial training process. It employs a Conditional GAN pattern, where a `kp_extractor` extracts keypoints (`kp_driving`, `kp_source`) that condition both the generator's output and the discriminator's evaluation. The generator's objective is further refined by a hybrid loss function, combining adversarial GAN loss with reconstruction losses to enhance generation quality."
monkey-net/cluster_15.py,Forecasting with Classical Models,"This code implements a motion transfer generator, utilizing an encoder-decoder architecture to synthesize target frames from a source image and keypoint-based motion. It employs a dense motion module to predict deformation fields from source and driving keypoints, which are then used to warp appearance features via spatial transformer-like operations. The generator further incorporates keypoint embeddings and a refinement module to produce both an initially warped frame and a high-quality, refined video prediction."
monkey-net/cluster_2.py,Forecasting with Classical Models,"This code implements a Generative Adversarial Network (GAN) training pattern, featuring distinct generator, discriminator, and a keypoint detector network. It employs a multi-component optimization strategy with separate Adam optimizers and learning rate schedulers for each network, including conditional updates for the keypoint detector. The training loop incorporates adversarial losses alongside reconstruction losses and utilizes data parallelism for efficient execution."
nuplan-devkit/cluster_96.py,Forecasting with Classical Models,"The code implements the Intelligent Driver Model (IDM) as a reactive car-following policy, dictating longitudinal acceleration based on ego and lead agent states, desired velocity, and safety parameters. This policy is central to an agent-based simulation framework, where `IDMAgent`s dynamically plan and propagate their trajectories by interacting with an `OccupancyMap` that represents other vehicles and traffic light statuses."
nuplan-devkit/cluster_98.py,Forecasting with Classical Models,"This code implements an `IDMPlanner` that utilizes the Intelligent Driver Model (IDM) for longitudinal vehicle control, representing a specific AI car-following policy. A core AI pattern is its path planning component, which employs a Breadth-First Search (BFS) algorithm to efficiently determine optimal routes through a lane graph structure. This demonstrates the application of a classic graph search algorithm for autonomous navigation."
oasis/cluster_17.py,Forecasting with Classical Models,"This code implements a personalized recommendation system utilizing **content-based filtering**, where user bios and post content are transformed into embeddings to compute semantic similarity. This base similarity is then refined through a **hybrid approach** that incorporates **user interaction traces** (likes and dislikes) to adjust scores, enhancing personalization. Furthermore, a **diversity promotion** mechanism randomly swaps a percentage of recommended posts to broaden exposure."
oasis/cluster_19.py,Forecasting with Classical Models,"The code demonstrates two distinct recommendation system patterns. It includes a basic random recommendation system, which serves as a baseline for diverse or cold-start recommendations. Additionally, it implements a content-based recommendation system that ranks posts using a Reddit-like ""hot score"" algorithm, prioritizing items based on their engagement metrics and recency."
optillm/cluster_38.py,Forecasting with Classical Models,"The code implements a Test-Time Diffusion Deep Researcher (TTD-DR) algorithm, treating research generation as an iterative diffusion process. This pattern involves generating a preliminary, ""noisy"" draft that is progressively refined through cycles of gap analysis, targeted information retrieval, and denoising by integrating new content. The iterative refinement is guided by quality evaluations, with a mechanism for self-evolutionary optimization of research components to adapt strategies."
optillm/cluster_39.py,Forecasting with Classical Models,"This code implements a Test-Time Diffusion Deep Researcher (TTD-DR) pattern, treating research as an iterative diffusion process. It employs a feedback loop where a preliminary draft is progressively denoised by analyzing gaps, performing targeted retrieval, and integrating new, cited information. This iterative refinement continues until a quality-guided termination condition is met, with an intended, though not fully active, component-wise self-evolutionary optimization mechanism."
optillm/cluster_6.py,Forecasting with Classical Models,"This code implements a transformer-based classification model (`OptILMClassifier`) to predict the optimal ""approach"" from a set of options. A key AI pattern is the multi-objective loss function, which combines cross-entropy for classification with an MSE loss that regularizes the model's confidence to align with a normalized ""effort"" metric. This allows the model to learn cost-aware preferences, predicting the best approach while reflecting its associated effort in the prediction confidence."
pomdp-baselines/cluster_10.py,Forecasting with Classical Models,"This code implements model-free off-policy reinforcement learning agents, primarily employing actor-critic architectures. It distinguishes between Markovian environments using MLPs and partially observable environments that leverage recurrent neural networks (RNNs) for state representation. All agents incorporate twin Q-functions and target networks for stable learning, with RNN-based models further varying between shared or separate recurrent encoders for the actor and critic."
pomdp-baselines/cluster_8.py,Forecasting with Classical Models,"This code implements a `TanhGaussianPolicy`, a common pattern in continuous control reinforcement learning for stochastic policies. It models actions as a Gaussian distribution whose outputs are squashed by a hyperbolic tangent function, enabling bounded action spaces. Crucially, it incorporates the reparameterization trick via `rsample()` to allow gradients to flow through the sampling process, a fundamental technique for training such policies in algorithms like Soft Actor-Critic (SAC)."
scattertext/cluster_26.py,Forecasting with Classical Models,"This code demonstrates a collection of AI patterns for statistical text analysis and information retrieval. It implements various methods for comparative analysis, including effect size calculations (Cohen's d, Hedges' g), Bayesian inference (Beta Posterior), and hypothesis testing (Z-Scores from Welch's T-Test). Additionally, it features information-theoretic measures like Frankhauser Relative Entropy and specialized term weighting schemes such as BM25 Difference and Cred-TFIDF for assessing term importance and document relevance across categories."
synthetic-data-generator/cluster_27.py,Forecasting with Classical Models,"This code implements a Conditional Generative Adversarial Network (GAN) pattern, specifically a CTGAN, for synthesizing tabular data. It features distinct Generator and Discriminator networks trained adversarially, incorporating a gradient penalty for stable learning. The pattern further includes conditional data generation and specialized handling of discrete features using Gumbel-Softmax activation and cross-entropy loss."
synthetic-data-generator/cluster_49.py,Forecasting with Classical Models,"The code implements an adaptive probabilistic modeling framework, featuring automated univariate distribution selection based on statistical fit (Kolmogorov-Smirnov test) and dynamic candidate filtering. It utilizes a Gaussian copula model for multivariate distributions, separating marginal distribution fitting from dependency modeling via a correlation matrix. This architecture supports conditional inference and sampling by transforming data into a standard normal space."
synthetic-data-generator/cluster_51.py,Forecasting with Classical Models,"The code implements advanced probabilistic modeling patterns, featuring an adaptive univariate distribution selection mechanism that dynamically fits the best marginal distribution. It further includes sophisticated multivariate generative models: a Gaussian Copula for separating marginals from dependency structures, and a hierarchical Vine Copula for modeling complex multivariate dependencies through a sequence of bivariate copulas. These patterns collectively enable dynamic model selection, conditional sampling, and robust data generation from learned distributions."
tf.fashionAI/cluster_20.py,Forecasting with Classical Models,"This code implements a Cascaded Pyramid Network (CPN) architecture, leveraging a ResNet-like backbone with bottleneck blocks and dilated convolutions for feature extraction. It constructs a Feature Pyramid Network (FPN) by combining multi-scale features through lateral connections and upsampling, generating intermediate heatmaps. A subsequent ""GlobalNet"" then aggregates these pyramid features to produce a refined final prediction."
tf.gans-comparison/cluster_2.py,Forecasting with Classical Models,"This code implements a Generative Adversarial Network (GAN) training and evaluation pipeline. It features a training loop with distinct discriminator and generator optimization steps, sampling from a latent space (`model.z`) to produce synthetic data. The evaluation routine further demonstrates generative model capabilities by producing and visualizing samples from trained checkpoints."
ChatSim/cluster_103.py,Forecasting with Classical Models,"This code implements a **Synchronized Batch Normalization** layer, a critical AI pattern for stabilizing and accelerating deep neural network training. It employs a **data-parallel computing pattern** where batch statistics (mean and variance) are aggregated across multiple devices using master-slave communication, reduction, and broadcast operations. This ensures consistent statistics calculation during distributed training, while also maintaining **moving averages** for robust inference."
AIlice/cluster_32.py,Evaluating LLM Results,"This code implements an AI agent evaluation framework, specifically designed for benchmarks like GAIA, by systematically processing dataset entries, constructing prompts, and obtaining responses from an `agent_function`. It incorporates robust answer normalization and type detection patterns to standardize AI model outputs (numbers, lists, strings) against ground truth, ensuring accurate evaluation. Furthermore, it includes patterns for structured prompt building and extracting specific answers from free-form AI responses, crucial for automated assessment."
ChatSim/cluster_8.py,Evaluating LLM Results,"This code implements a specialized database system for managing core data structures in computer vision, particularly for 3D reconstruction pipelines like Structure-from-Motion. It defines patterns for storing camera parameters, image poses, local image features (keypoints and descriptors), and their correspondences (matches and inlier matches). This facilitates the persistent storage and retrieval of geometric and photometric information essential for multi-view geometry and scene understanding tasks."
ChatSim/cluster_92.py,Evaluating LLM Results,"This code implements a comprehensive evaluation framework for generative AI models, prominently featuring the Frechet Inception Distance (FID) metric. It utilizes a pre-trained InceptionV3 model for robust feature extraction and calculates statistical distances between generated and real image distributions. The framework further extends to segmentation-aware evaluation, enabling class-wise performance analysis and attribution of FID errors to specific semantic categories."
DLTA-AI/cluster_21.py,Evaluating LLM Results,"This code implements robust AI training and evaluation patterns, focusing on multi-device compatibility and distributed processing. It features dynamic learning rate scaling, mixed-precision training (FP16), and comprehensive evaluation hooks for metrics like mAP, supporting various hardware accelerators (CUDA, NPU, MLU). Additionally, it includes utilities for analyzing model robustness against data corruptions across different datasets (COCO, VOC)."
EvoAgentX/cluster_10.py,Evaluating LLM Results,"This code implements a robust benchmarking framework for evaluating AI models across diverse tasks. It defines distinct benchmark classes for language understanding (BIGBenchHard), code generation (MBPP, HumanEval), question answering (HotPotQA, NQ), and mathematical reasoning (GSM8K). The core AI patterns include automated dataset management, handling downloading and loading of specific AI benchmark datasets, and providing task-specific evaluation logic with metrics like exact match, F1 score, pass@k, and solve rate."
EvoAgentX/cluster_13.py,Evaluating LLM Results,"This code tests an `MBPP` benchmark, demonstrating AI patterns for loading and structuring a code generation dataset. It verifies the extraction of ground truth labels, including canonical solutions and test cases, and the computation of `pass@1` as a key evaluation metric for program synthesis models."
EvoAgentX/cluster_32.py,Evaluating LLM Results,"This code implements an AI benchmarking pattern, specifically for evaluating models on the BIGBenchHard dataset. It manages task-specific data loading, structures examples with distinct ""input"" and ""target"" fields, and implements data splitting for development and testing. Furthermore, it employs an exact match evaluation pattern, a common metric for generative AI tasks, and ensures reproducibility through explicit random seed setting."
factorio-learning-environment/cluster_74.py,Evaluating LLM Results,"This code implements a robust AI experiment management pattern, orchestrating large-scale evaluation sweeps for various models and tasks with features like resuming, retrying failed runs, and concurrent execution. A core AI optimization pattern is dynamic early stopping, which halts further evaluations of a (model, task) pair once a task-specific success criterion, such as a throughput quota, is met. The system further supports detailed performance evaluation by defining success metrics and integrating with experiment tracking for comprehensive analysis of AI model trajectories."
mAP/cluster_0.py,Evaluating LLM Results,"This code implements a visualization pattern for evaluating AI model performance, specifically focusing on classification or object detection tasks. It generates stacked horizontal bar charts to clearly represent True Positives and False Positives. This pattern facilitates the analysis of model predictions against ground-truth data, providing a visual breakdown of correct and incorrect detections."
nuplan-devkit/cluster_111.py,Evaluating LLM Results,"This code exemplifies AI patterns in autonomous driving scenario management and data curation. It implements sophisticated filtering utilities to select and balance scenario datasets based on criteria like total count, temporal proximity, LiDAR token overlap, and ego vehicle motion characteristics (e.g., starts, stops, stationarity). These patterns are crucial for preparing diverse and relevant data for training and evaluating perception and planning models in self-driving systems."
nuplan-devkit/cluster_156.py,Evaluating LLM Results,"This code implements a pattern for comparative performance evaluation of AI planners. It aggregates metric scores and scenario counts from experimental data, dynamically generating a table that displays side-by-side comparisons of different AI planner names across various scenario types. This structure facilitates benchmarking and analysis of AI model performance, including interactive filtering of specific planners."
optillm/cluster_2.py,Evaluating LLM Results,"This code implements an LLM evaluation framework, systematically benchmarking models against problems using configurable approaches like 'mars' or 'moa'. It demonstrates AI patterns for dynamic prompt construction tailored to problem types and programmatic evaluation of LLM responses for correctness. Additionally, it includes a pattern for ranking multiple LLM outputs based on correctness and token efficiency."
synthetic-data-generator/cluster_42.py,Evaluating LLM Results,"This code implements a pattern for generating diverse synthetic datasets, spanning categorical, numerical, and datetime types, with a primary AI focus on systematically injecting missing values (NaNs). This simulation of real-world data imperfections is crucial for evaluating the robustness of AI models and developing imputation strategies. Each generator also defines performance thresholds, indicating a pattern for benchmarking data processing operations within an AI pipeline."
ladi-vton/cluster_7.py,Evaluating LLM Results,"This code implements a comprehensive evaluation pipeline for generative AI models, specifically focusing on image generation tasks. It employs a suite of advanced metrics like FID, KID, Inception Score, LPIPS, and SSIM to assess both the perceptual quality and the statistical similarity of generated image distributions against real datasets. A notable pattern is the pre-computation and caching of dataset statistics for FID and KID, optimizing the efficiency of distributional metric calculations across various categories and datasets."
DLTA-AI/cluster_32.py,Evaluating LLM Results,"This code implements an MLOps integration pattern, specifically an enhanced Weights & Biases logger hook for MMDetection. It facilitates comprehensive experiment tracking by automatically logging training and validation metrics, versioning model checkpoints as W&B Artifacts with associated evaluation metadata. Furthermore, it visualizes model predictions, including bounding boxes and masks, as interactive W&B Tables for detailed qualitative analysis of AI model performance."
AgentBench/cluster_4.py,Evaluating LLM Results,"This code implements an AI evaluation framework designed to systematically assess the performance of various AI agents across multiple tasks. It processes structured experiment outputs, extracting main performance metrics via task-specific handlers and aggregating validation results. The system then generates comprehensive reports, enabling detailed comparison of agent performance and validation outcomes."
EvoAgentX/cluster_59.py,Evaluating LLM Results,"This code outlines a testing framework for evaluating AI models on mathematical reasoning and problem-solving tasks, leveraging a structured dataset with problems and detailed solutions. It demonstrates patterns for extracting final answers from complex mathematical solutions and employs specialized equality functions (`math_equal`, `symbolic_equal`) to rigorously assess the correctness of AI-generated predictions, yielding a `solve_rate` metric."
HuixiangDou/cluster_2.py,Evaluating LLM Results,"The code implements AI patterns focused on serving, managing, and evaluating Large Language Models (LLMs). It features a `HybridLLMServer` for streaming and non-streaming inference, a `FeatureStore` for managing embedded data, and a `KnowledgeGraph` for retrieval-augmented generation. Additionally, it includes evaluation mechanisms using F1, precision, and recall scores to assess the performance of these AI components."
Automatic_Speech_Recognition/cluster_17.py,"Enabling Reliability, Explainability, or Robustness","This code exemplifies a **rule-based Natural Language Processing (NLP) pattern** for text normalization. It systematically identifies and transforms various numerical entities—decimals, integers, and year-specific numbers—within a given string into their Chinese textual equivalents. The pattern leverages regular expressions for precise entity extraction and applies distinct, predefined conversion rules for each number type, such as converting ""2018年"" to ""二年一八年""."
ChatSim/cluster_124.py,"Enabling Reliability, Explainability, or Robustness","This code implements a pattern for integrating Large Language Models (LLMs) by dynamically executing Python code generated by an LLM. It extracts code from an LLM's response, writes it to a temporary file, and executes it via a system call, subsequently parsing the structured output (e.g., coordinates) using `ast.literal_eval`. This enables LLMs to generate executable logic for tasks like trajectory generation."
FinceptTerminal/cluster_56.py,"Enabling Reliability, Explainability, or Robustness","This code exemplifies a multi-model prediction strategy, leveraging distinct machine learning algorithms like Random Forest, Gradient Boosting, Neural Networks, and LSTMs. A core pattern is ensemble learning, where predictions from these diverse models are combined to form a more robust `ensemble_prediction`. Furthermore, it emphasizes model robustness and interpretability through the calculation of `model_confidence`, `model_agreement`, and the extraction of `feature_importance`."
MassGen/cluster_11.py,"Enabling Reliability, Explainability, or Robustness","The code implements critical security AI patterns for validating and sanitizing external inputs and configurations, crucial for preventing malicious execution in AI-driven systems. It leverages extensive input validation, allowlisting for executables and hostnames, and denylisting for dangerous ports and environment variables to mitigate command injection, SSRF, and path traversal vulnerabilities. These patterns are further enhanced by configurable security levels, enabling adaptive policy enforcement for secure AI operational environments."
PointTransformerV2/cluster_18.py,"Enabling Reliability, Explainability, or Robustness","This code implements a robust 3D data preprocessing pipeline, calculating essential geometric features like vertex and face normals from mesh data. It further focuses on generating comprehensive ground truth labels for 3D semantic and instance segmentation tasks by mapping raw scene annotations to standardized class IDs. The processed data, including features and ground truth, is then saved in PyTorch format, preparing structured datasets for training deep learning models on 3D point clouds or meshes."
adaptive-classifier/cluster_9.py,"Enabling Reliability, Explainability, or Robustness","The code demonstrates AI patterns for optimizing model inference, specifically by integrating ONNX Runtime with pre-trained transformer models like `bert-tiny` within an `AdaptiveClassifier`. It showcases robust strategies for ensuring output consistency between ONNX and PyTorch implementations, alongside adaptive deployment mechanisms that automatically utilize ONNX when available or gracefully fall back to PyTorch. This approach prioritizes both performance efficiency and operational reliability for AI model deployment and integration into adaptive learning workflows."
optillm/cluster_45.py,"Enabling Reliability, Explainability, or Robustness","The code defines configuration validation for an AI system, ""DeepConf,"" revealing patterns common in generative AI and sequence processing. It manages parameters like `temperature` for controlling output randomness, `top_k` for constrained sampling or prediction, and `max_tokens_per_trace` for handling sequence lengths. These patterns indicate a system designed for tasks involving token generation, trace-based execution, and potentially consensus-driven decision making."
optillm/cluster_63.py,"Enabling Reliability, Explainability, or Robustness","This code demonstrates patterns for testing Language Model (LLM) interactions, specifically focusing on generating multiple completions using the `n` parameter and enforcing structured output through JSON schema. It also includes patterns for verifying the retrieval of token usage details from LLM responses and for setting up local inference environments that mimic the OpenAI API."
AIlice/cluster_1.py,Agent Architecture,"This code defines a multi-agent AI system where a central `AProcessor` orchestrates specialized prompts, such as `APromptChat`, `APromptCoder`, and `APromptSearchEngine`, to handle diverse tasks. It extensively utilizes external tools and services, including search engines, code interpreters, and a vector database for retrieval-augmented generation, to extend its capabilities. The system supports a dynamic, conversational user experience, further enhanced by integrated multimodal speech recognition and text-to-speech modules."
AIlice/cluster_3.py,Agent Architecture,"This code defines a multi-agent AI system where specialized agents, like `APromptResearcher` and `APromptCoderProxy`, are equipped with extensive tool-use capabilities, dynamically invoking functions such as `BROWSE`, `BASH`, `PYTHON`, and `ARXIV`. The system employs retrieval-augmented generation (RAG) by recalling relevant information from persistent storage to inform agent actions and adaptively constructs prompts to manage context window limitations. This architecture facilitates complex task execution through agent collaboration and dynamic function discovery."
AIlice/cluster_34.py,Agent Architecture,"This code implements an agent-based AI pattern, where `AProcessor` acts as a conversational agent managing its own LLM, prompt, and context. It features a robust tool-use mechanism via an `AInterpreter` that parses LLM outputs to execute registered actions, including dynamic loading of external modules and sub-agents. This enables complex function calling, interaction with services, and hierarchical agent orchestration."
AIlice/cluster_8.py,Agent Architecture,"This code implements a multi-agent AI architecture, utilizing specialized `APrompt` types like 'researcher' and 'coder' managed by an `AProcessor`. It orchestrates conversational AI sessions, handling multimodal inputs (e.g., images, audio) and integrating various external tools and LLMs via `AClientPool` and `ALLMPool`. State machines within `UserContext` and `TaskSession` govern the lifecycle and workflow of these complex AI interactions."
EvoAgentX/cluster_60.py,Agent Architecture,"The code implements an agentic workflow orchestration pattern, where a `WorkFlowGenerator` dynamically constructs multi-step AI workflows and agents based on a high-level goal. It extensively uses Large Language Models (LLMs) as tools for specific actions, including generating code, verifying its adherence to requirements via a `CodeVerification` action, and extracting structured code blocks from LLM outputs. This architecture facilitates dynamic AI-driven development with structured LLM interactions."
EvoAgentX/cluster_65.py,Agent Architecture,"The code demonstrates a pattern for building highly customizable AI agents (`CustomizeAgent`) that integrate external tools (e.g., `MCPToolkit`, `ArxivToolkit`) to augment their capabilities. A core AI pattern is the robust handling of structured output through various parsing modes (`json`, `xml`, `title`, `custom`), ensuring precise information extraction. Furthermore, it showcases the orchestration of these agents and tools into complex workflows (`WorkFlowGraph`) for multi-step task execution, representing a multi-agent system pattern."
FinceptTerminal/cluster_53.py,Agent Architecture,"The code implements a pattern of specialized AI agents, each meticulously crafted with a distinct investment philosophy and reasoning style, mimicking renowned hedge funds like Bridgewater or Renaissance. These agents leverage highly structured system prompts to define their organizational structure, philosophy, and decision-making process. Robust LLM interaction is ensured through structured output (Pydantic models), retry mechanisms, and default factories, enabling reliable and type-safe generation of investment signals."
GoBigger/cluster_13.py,Agent Architecture,"The code demonstrates distinct AI patterns for game agents, including basic rule-based `BotAgent`s for simple adversaries. More advanced `AIAgent` implementations are present, differentiating between a `solo_agent` designed for individual competitive play and a `cooperative_agent` capable of team-based strategic interaction. These patterns facilitate scenarios from human-AI gameplay to full AI-vs-AI simulations, showcasing varying levels of autonomous decision-making and collaboration."
MassGen/cluster_2.py,Agent Architecture,"The code implements a multi-agent orchestration pattern where an `Orchestrator` coordinates `ChatAgent` instances using a consensus-based decision framework, allowing agents to `vote` on solutions or propose `new_answer`s. This system dynamically manages and shares agent states, including workspace snapshots and conversation history, while integrating external capabilities through a Model Context Protocol (MCP) for enhanced problem-solving."
MassGen/cluster_34.py,Agent Architecture,"This code implements multi-agent orchestration patterns, specifically supporting AutoGen's `AutoPattern` for structured group chats. It employs a role-based collaboration pattern, where a dedicated `user_agent` handles final decisions and workflow tool execution, distinct from other expert agents. A staged coordination pattern is also evident, dynamically adjusting the `user_agent`'s system message and tool access across distinct phases of group chat execution."
PentestAgent/cluster_2.py,Agent Architecture,"The code implements an agent-based architecture, orchestrating interactive, automated workflow, and autonomous agent modes. It integrates a knowledge base for contextual understanding and employs a conversation manager to maintain dialogue history for AI interactions. A prominent autonomous agent pattern is evident through the `AgentModeController`, which manages goal-driven execution, leverages external tools, and generates structured reports from its operational history."
clai/cluster_2.py,Agent Architecture,"This code implements an agent-based system where an `AgentRunner` orchestrates multiple `Agent` instances, selecting optimal actions via an `OrchestratorProvider` based on the current `State`. It employs a data collection pattern, storing `TerminalReplayMemory` of user interactions (states, candidate actions, suggested commands) both pre and post-execution, which is foundational for reinforcement learning or model refinement. Additionally, `StatsTracker` and `ActionRemoteStorage` gather anonymized telemetry and usage data to monitor system performance and inform future AI enhancements."
clai/cluster_3.py,Agent Architecture,"This code implements an intelligent assistant pattern, utilizing a `MessageHandler` to interpret user commands and manage system states like 'power mode'. It employs an agent-based plugin architecture, where various AI `Agent` instances can be dynamically selected to process commands. A confidence-based arbitration mechanism is used to decide whether to adopt an agent's suggested command, ensuring intelligent decision-making based on the AI's certainty."
evolving-agents/cluster_6.py,Agent Architecture,"This code implements an **Agent-Oriented Architecture** through a `SmartAgentBus` that manages the registration, discovery, and execution of various AI agents and tools. It leverages **semantic search** using vector embeddings for dynamic agent discovery based on natural language task descriptions. Furthermore, it incorporates **LLM caching** for efficiency and **circuit breaker patterns** to enhance system resilience by preventing repeated calls to failing agents."
factorio-learning-environment/cluster_14.py,Agent Architecture,"This code implements a reactive control mechanism for an AI agent, processing external hook events and messages to determine subsequent actions. It employs pattern matching against predefined `auto_continue_patterns` to autonomously resume the agent's operation upon detecting specific completion or waiting states. Additionally, it identifies and relays warnings or errors from the agent's output, facilitating a feedback loop for monitoring and potential intervention."
factorio-learning-environment/cluster_21.py,Agent Architecture,"This code defines an OpenAI Gym environment (`FactorioGymEnv`) for a Factorio simulation, explicitly supporting multi-agent reinforcement learning. Agents interact by submitting programmatic actions (Lua code) to manipulate the game state, receiving rich observations that include inter-agent messages. The system incorporates task-specific goal verification and robust state serialization, crucial for training and evaluating AI agents in complex, dynamic environments."
factorio-learning-environment/cluster_68.py,Agent Architecture,"This code defines an AI environment characterized by a comprehensive, multimodal observation space, providing agents with detailed symbolic data (inventory, entities, research, game state, messages, raw text, serialized functions) and perceptual data (base64 encoded map images, formatted for multimodal APIs). Agents interact with this environment through a programmatic action space, executing Python code snippets to perform complex actions like crafting, movement, and entity placement. The environment meticulously tracks temporal dynamics and resource flows, facilitating AI patterns focused on planning, resource management, and code generation for complex tasks."
optillm/cluster_1.py,Agent Architecture,"The code implements sophisticated multi-agent collaboration and meta-reasoning patterns, featuring recursive solution aggregation, self-critique mechanisms, and a Strategy Network for extracting and sharing reasoning approaches among agents. It extensively leverages Large Language Models (LLMs) as expert verifiers for rigorous, two-stage solution evaluation (IMO25-style grading) and integrates LLM-driven planning through Monte Carlo Tree Search for dynamic decision-making in dialogue states. Additionally, robust information extraction and normalization techniques are employed to process and compare LLM-generated outputs effectively."
optillm/cluster_49.py,Agent Architecture,"The code implements a Multi-Agent Reasoning System (MARS) pattern, orchestrating multiple AI agents to solve complex problems. It leverages parallel execution of Large Language Models (LLMs) to explore diverse reasoning paths, followed by a consensus and verification mechanism, including voting and synthesis, to produce a robust final solution. This architecture enables sophisticated problem-solving by combining multiple perspectives and iterative refinement."
traceroot/cluster_8.py,Agent Architecture,"This code implements an AI agent orchestration pattern, utilizing a `ChatRouter` to dynamically route user queries to specialized agents (e.g., RCA, Code, General) based on contextual analysis of the message and related GitHub/observability data. It leverages LLMs for tasks like summarization, relevance detection, and generating conversational responses, enabling agents to reason with traces, logs, and source code, and facilitating user confirmation for AI-proposed actions like GitHub issue or PR creation."
EvoAgentX/cluster_85.py,Advanced LLM Prompting,"This code demonstrates an AI pattern for orchestrating multi-hop Question Answering (QA) workflows, integrating a Large Language Model (LLM) and employing modular operators like `AnswerGenerate` and `QAScEnsemble`. The `QAScEnsemble` operator suggests a self-consistency or ensemble technique to enhance QA robustness, while the `ActionGraph` structure facilitates defining, serializing, and loading these configurable AI workflows."
GPT_Vuln-analyzer/cluster_12.py,Advanced LLM Prompting,"This code employs an AI pattern where an OpenAI Large Language Model (LLM) is utilized for expert analysis of raw NMAP scan data. It leverages detailed prompt engineering to instruct the LLM to perform security analysis from a pentester's perspective and generate structured JSON output. The LLM's response is then post-processed using regular expressions to reliably extract specific security insights such as critical scores, vulnerabilities, and open services."
HuixiangDou/cluster_68.py,Advanced LLM Prompting,"The code demonstrates patterns for interacting with large language models (LLMs) via a chat completion API, including model enumeration and controlled text generation with parameters like temperature and multiple response sampling. A key AI pattern is the implementation of a ""needle in a haystack"" evaluation, where prompts are programmatically constructed to embed specific information within extensive irrelevant text. This evaluation rigorously tests the LLM's ability to retrieve and recall specific facts from long, noisy contexts, with performance measured by precision."
MassGen/cluster_29.py,Advanced LLM Prompting,"The code demonstrates a dynamic context building pattern for AI conversations, meticulously constructing LLM input messages that evolve across multiple turns by integrating conversation history. It employs a structured prompting approach, segmenting the user message to include the current task, historical exchanges, and summaries from other AI agents (`CURRENT ANSWERS`). This comprehensive context also prepares tool definitions, enabling the AI to leverage external functions for enhanced capabilities."
Stride-AI-Agents/cluster_4.py,Advanced LLM Prompting,"The code primarily showcases an AI pattern of **Large Language Model (LLM) integration for intelligent content extraction**. It leverages an `LLMExtractionStrategy` with models like `openai/gpt-4o` to process web page content beyond simple parsing. This pattern is further enhanced by **prompt engineering**, allowing users to provide specific instructions to the LLM for highly targeted information retrieval based on defined criteria."
factorio-learning-environment/cluster_11.py,Advanced LLM Prompting,"This code implements an AI pattern for structured content generation and subsequent contrastive evaluation. It first leverages an LLM to generate a blueprint's title and purpose, extracting this information via regex from a structured JSON output. This generated content then forms the basis for creating multiple-choice contrastive matching questions, where the correct answer is presented alongside predefined distractors for identification tasks."
optillm/cluster_31.py,Advanced LLM Prompting,"This code implements a privacy-preserving AI pattern by integrating Named Entity Recognition (NER) and anonymization with Large Language Model (LLM) interaction. It preprocesses user queries to anonymize sensitive entities before sending them to the LLM, and subsequently de-anonymizes the LLM's response. A singleton pattern is employed to efficiently manage and reuse expensive AI resources, such as the analyzer and anonymizer engines, optimizing performance for repeated invocations."
pybroker/cluster_1.py,Advanced LLM Prompting,"The code implements a backtesting framework that integrates pre-trained AI models for generating trading signals and informing strategy execution. It employs a structured data scoping pattern, including `ModelInputScope` and `PredictionScope`, to prepare and manage data for AI model consumption and prediction generation. This architecture facilitates prediction-driven decision-making for trade placement and position sizing within the simulated trading environment."
traceroot/cluster_28.py,Advanced LLM Prompting,"This code implements the **function calling** AI pattern by generating an OpenAI JSON schema from a given Python function. This schema serves as a structured description, enabling large language models (LLMs) to understand the function's purpose, parameters, and how to invoke it. It specifically facilitates **tool use** by translating Python function interfaces into a format consumable by OpenAI's API for AI agent interaction."
EvoAgentX/cluster_63.py,Advanced LLM Prompting,"This code demonstrates an **Agent-based architecture** where `ActionAgent` instances encapsulate specific functionalities with clearly defined input/output schemas, facilitating modular and self-describing computational units. It showcases key AI patterns such as **function/tool registration**, **asynchronous execution**, and robust **input validation and error handling** for reliable operation. The ability to **serialize and deserialize agents** further supports their persistence and dynamic management within an AI system."
EvoAgentX/cluster_47.py,Advanced LLM Prompting,"This code implements robust AI output parsing and correction patterns, featuring sophisticated JSON extraction and repair mechanisms (`fix_json`, `parse_json_from_text`) to handle common LLM generation imperfections across various formats (JSON, XML, title). A prominent pattern is the integration of tool-use capabilities within `CustomizeAction`, enabling LLMs to dynamically call external functions by extracting structured tool calls from their responses, executing them, and incorporating results into the ongoing dialogue. The system emphasizes structured output generation via Pydantic models and includes a fallback mechanism where the LLM itself is prompted to extract structured data if initial parsing attempts fail."
EvoAgentX/cluster_48.py,Advanced LLM Prompting,"This code implements robust patterns for **LLM output parsing and structured data extraction**, converting raw LLM-generated text into defined Pydantic models using various strategies like JSON, XML, or markdown titles. It further supports **LLM agentic workflows** by dynamically defining and validating action input schemas, enabling reliable **tool/function calling** where LLMs generate structured inputs for subsequent operations."
EvoAgentX/cluster_54.py,Advanced LLM Prompting,"This code implements a robust pattern for extracting structured data from raw Large Language Model (LLM) outputs. It provides multiple parsing strategies, including JSON, XML, and Markdown-style title extraction, to accommodate diverse LLM response formats. This design enables converting unstructured LLM text into predefined attribute-value pairs for further processing."
mindpalace/cluster_4.py,Advanced LLM Prompting,"This code demonstrates the integration of a Large Language Model (LLM), specifically Google's Gemini-2.0-flash, for advanced text analysis. It utilizes sophisticated prompt engineering to guide the LLM in extracting logical and hierarchical relationships from topic summaries, ensuring structured JSON output. This AI-generated information then drives dynamic content creation, forming Mermaid diagram code for visualization."